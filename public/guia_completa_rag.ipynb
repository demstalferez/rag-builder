{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG (Retrieval-Augmented Generation)\n",
    "## Ãndice de Contenidos\n",
    "\n",
    "1. [Fundamentos TeÃ³ricos de RAG](#1-fundamentos)\n",
    "2. [Embeddings: El CorazÃ³n SemÃ¡ntico](#2-embeddings)\n",
    "3. [Chunking: DivisiÃ³n Inteligente de Documentos](#3-chunking)\n",
    "4. [Vector Stores: Almacenamiento y BÃºsqueda](#4-vectorstores)\n",
    "5. [Pipeline RAG Completo](#5-pipeline)\n",
    "6. [TÃ©cnicas Avanzadas de Retrieval](#6-avanzadas)\n",
    "7. [EvaluaciÃ³n y MÃ©tricas](#7-evaluacion)\n",
    "8. [Prompt Engineering para RAG](#8-prompting)\n",
    "9. [Soluciones Open Source](#9-opensource)\n",
    "10. [Despliegue en ProducciÃ³n](#10-produccion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ConfiguraciÃ³n del Entorno\n",
    "\n",
    "Antes de comenzar, necesitamos instalar las bibliotecas necesarias. Este notebook utiliza herramientas open source que pueden ejecutarse completamente en local, sin necesidad de APIs externas de pago.\n",
    "\n",
    "### Bibliotecas principales:\n",
    "\n",
    "- **sentence-transformers**: Modelos de embeddings de Ãºltima generaciÃ³n\n",
    "- **chromadb**: Base de datos vectorial ligera y fÃ¡cil de usar\n",
    "- **faiss-cpu**: Biblioteca de Facebook para bÃºsqueda de similitud eficiente\n",
    "- **langchain**: Framework para construir aplicaciones con LLMs\n",
    "- **transformers**: Biblioteca de Hugging Face para modelos de lenguaje\n",
    "- **rank_bm25**: ImplementaciÃ³n del algoritmo BM25 para bÃºsqueda lÃ©xica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InstalaciÃ³n de dependencias (ejecutar solo la primera vez)\n",
    "!pip install -q \\\n",
    "    langchain \\\n",
    "    langchain-community \\\n",
    "    langchain-huggingface \\\n",
    "    sentence-transformers \\\n",
    "    chromadb \\\n",
    "    faiss-cpu \\\n",
    "    transformers \\\n",
    "    torch \\\n",
    "    pypdf \\\n",
    "    rank_bm25 \\\n",
    "    scikit-learn \\\n",
    "    matplotlib \\\n",
    "    numpy \\\n",
    "    pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports principales que usaremos a lo largo del notebook\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(\"âœ… Entorno configurado correctamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"1-fundamentos\"></a>\n",
    "# 1. Fundamentos TeÃ³ricos de RAG\n",
    "\n",
    "## 1.1 Â¿QuÃ© es RAG?\n",
    "\n",
    "**RAG (Retrieval-Augmented Generation)** es una arquitectura que combina dos componentes fundamentales:\n",
    "\n",
    "1. **Retrieval (RecuperaciÃ³n)**: Un sistema que busca y recupera informaciÃ³n relevante de una base de conocimiento\n",
    "2. **Generation (GeneraciÃ³n)**: Un modelo de lenguaje (LLM) que genera respuestas basÃ¡ndose en la informaciÃ³n recuperada\n",
    "\n",
    "Esta combinaciÃ³n permite que los LLMs accedan a informaciÃ³n actualizada, especÃ­fica y verificable, superando sus limitaciones inherentes.\n",
    "\n",
    "## 1.2 Â¿Por quÃ© necesitamos RAG?\n",
    "\n",
    "Los Modelos de Lenguaje Grande (LLMs) como GPT, Llama o Mistral tienen limitaciones importantes:\n",
    "\n",
    "### Problemas de los LLMs sin RAG:\n",
    "\n",
    "| Problema | DescripciÃ³n | Impacto |\n",
    "|----------|-------------|----------|\n",
    "| **Conocimiento desactualizado** | El modelo solo conoce informaciÃ³n hasta su fecha de entrenamiento | No puede responder sobre eventos recientes |\n",
    "| **Alucinaciones** | Genera informaciÃ³n plausible pero falsa con total confianza | Respuestas incorrectas presentadas como hechos |\n",
    "| **Sin acceso a datos privados** | No conoce documentos internos de tu organizaciÃ³n | No puede ayudar con informaciÃ³n especÃ­fica |\n",
    "| **Falta de citaciones** | No puede indicar de dÃ³nde proviene la informaciÃ³n | Imposible verificar las respuestas |\n",
    "| **Contexto limitado** | Ventana de contexto finita (4K-128K tokens) | No puede procesar grandes volÃºmenes de documentos |\n",
    "\n",
    "### CÃ³mo RAG resuelve estos problemas:\n",
    "\n",
    "| Problema | SoluciÃ³n con RAG |\n",
    "|----------|------------------|\n",
    "| Conocimiento desactualizado | Acceso a documentos actualizados en tiempo real |\n",
    "| Alucinaciones | Respuestas fundamentadas en fuentes especÃ­ficas |\n",
    "| Sin datos privados | IntegraciÃ³n con repositorios documentales internos |\n",
    "| Falta de citaciones | Referencias exactas a documentos y pÃ¡ginas |\n",
    "| Contexto limitado | RecuperaciÃ³n selectiva de informaciÃ³n relevante |\n",
    "\n",
    "## 1.3 Arquitectura de un Sistema RAG\n",
    "\n",
    "Un sistema RAG tÃ­pico consta de dos fases principales:\n",
    "\n",
    "### Fase 1: IndexaciÃ³n (Offline)\n",
    "\n",
    "Esta fase se ejecuta una vez (o periÃ³dicamente) para preparar los documentos:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ DOCUMENTOS  â”‚â”€â”€â”€â”€â–¶â”‚  CHUNKING   â”‚â”€â”€â”€â”€â–¶â”‚  EMBEDDING  â”‚â”€â”€â”€â”€â–¶â”‚VECTOR STORE â”‚\n",
    "â”‚ (PDF, DOCX, â”‚     â”‚ (DivisiÃ³n   â”‚     â”‚ (ConversiÃ³n â”‚     â”‚(Almacena-   â”‚\n",
    "â”‚  TXT, HTML) â”‚     â”‚  en partes) â”‚     â”‚ a vectores) â”‚     â”‚ miento)     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "1. **Carga de documentos**: Se leen los documentos en diversos formatos\n",
    "2. **Chunking**: Se dividen en fragmentos manejables (tÃ­picamente 500-1000 tokens)\n",
    "3. **Embedding**: Cada fragmento se convierte en un vector numÃ©rico que captura su significado\n",
    "4. **Almacenamiento**: Los vectores se guardan en una base de datos vectorial con Ã­ndices para bÃºsqueda rÃ¡pida\n",
    "\n",
    "### Fase 2: Consulta (Online)\n",
    "\n",
    "Esta fase se ejecuta cada vez que un usuario hace una pregunta:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  CONSULTA   â”‚â”€â”€â”€â”€â–¶â”‚  EMBEDDING  â”‚â”€â”€â”€â”€â–¶â”‚  BÃšSQUEDA   â”‚\n",
    "â”‚ del usuario â”‚     â”‚ de consulta â”‚     â”‚  VECTORIAL  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜\n",
    "                                               â”‚\n",
    "                                               â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  RESPUESTA  â”‚â—€â”€â”€â”€â”€â”‚     LLM     â”‚â—€â”€â”€â”€â”€â”‚  CONTEXTO   â”‚\n",
    "â”‚ al usuario  â”‚     â”‚ (GeneraciÃ³n)â”‚     â”‚ (Top-K docs)â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "1. **Embedding de consulta**: La pregunta del usuario se convierte en vector\n",
    "2. **BÃºsqueda**: Se encuentran los K fragmentos mÃ¡s similares semÃ¡nticamente\n",
    "3. **ConstrucciÃ³n de contexto**: Se prepara un prompt con los documentos recuperados\n",
    "4. **GeneraciÃ³n**: El LLM genera una respuesta basada en el contexto\n",
    "5. **Respuesta**: Se devuelve la respuesta con las fuentes citadas\n",
    "\n",
    "## 1.4 Componentes Clave\n",
    "\n",
    "### Modelo de Embeddings\n",
    "\n",
    "Convierte texto en vectores numÃ©ricos de alta dimensiÃ³n (tÃ­picamente 384-4096 dimensiones). Textos con significado similar producen vectores cercanos en el espacio vectorial.\n",
    "\n",
    "**Ejemplos de modelos:**\n",
    "- `BAAI/bge-m3` (Estado del arte, multilingÃ¼e)\n",
    "- `sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2` (Ligero, multilingÃ¼e)\n",
    "- `intfloat/multilingual-e5-large` (Excelente para espaÃ±ol)\n",
    "\n",
    "### Base de Datos Vectorial\n",
    "\n",
    "Almacena los vectores y permite bÃºsquedas eficientes por similitud.\n",
    "\n",
    "**Ejemplos:**\n",
    "- **ChromaDB**: Simple, ideal para prototipos\n",
    "- **FAISS**: Alto rendimiento, desarrollado por Facebook\n",
    "- **Qdrant**: ProducciÃ³n, filtros avanzados\n",
    "- **Milvus**: Escalabilidad masiva\n",
    "\n",
    "### Modelo de Lenguaje (LLM)\n",
    "\n",
    "Genera la respuesta final basÃ¡ndose en el contexto recuperado.\n",
    "\n",
    "**Modelos Open Source recomendados:**\n",
    "- **Llama 3.1** (8B-70B): PropÃ³sito general, excelente rendimiento\n",
    "- **Mistral/Mixtral**: Eficiente, buen razonamiento\n",
    "- **Qwen 2.5**: Excelente soporte multilingÃ¼e\n",
    "\n",
    "## 1.5 Ventajas de RAG para InvestigaciÃ³n CientÃ­fica\n",
    "\n",
    "Para instituciones como el CSIC, RAG ofrece ventajas especÃ­ficas:\n",
    "\n",
    "1. **SoberanÃ­a de datos**: Los documentos nunca salen de la infraestructura propia\n",
    "2. **Reproducibilidad**: Las respuestas son verificables y citables\n",
    "3. **ActualizaciÃ³n continua**: Se pueden aÃ±adir nuevas publicaciones sin reentrenar\n",
    "4. **Cumplimiento normativo**: Compatible con RGPD al no enviar datos a terceros\n",
    "5. **PersonalizaciÃ³n**: Adaptable a terminologÃ­a y dominios especÃ­ficos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"2-embeddings\"></a>\n",
    "# 2. Embeddings: El CorazÃ³n SemÃ¡ntico de RAG\n",
    "\n",
    "## 2.1 Â¿QuÃ© son los Embeddings?\n",
    "\n",
    "Los **embeddings** son representaciones numÃ©ricas (vectores) de texto que capturan el **significado semÃ¡ntico**. A diferencia de mÃ©todos tradicionales como TF-IDF o bag-of-words, los embeddings:\n",
    "\n",
    "- Capturan **relaciones semÃ¡nticas** entre palabras y frases\n",
    "- Representan el **significado** mÃ¡s allÃ¡ de las palabras exactas\n",
    "- Permiten **comparar similitud** entre textos de forma matemÃ¡tica\n",
    "\n",
    "### IntuiciÃ³n Visual\n",
    "\n",
    "Imagina un espacio de muchas dimensiones donde cada punto representa un texto:\n",
    "\n",
    "- \"El gato duerme en el sofÃ¡\" y \"El felino descansa en el sillÃ³n\" estarÃ­an **cerca** (mismo significado)\n",
    "- \"La proteÃ­na se pliega\" y \"El gato duerme\" estarÃ­an **lejos** (significados diferentes)\n",
    "\n",
    "### Propiedades MatemÃ¡ticas\n",
    "\n",
    "Los embeddings tienen propiedades algebraicas interesantes:\n",
    "\n",
    "```\n",
    "vector(\"rey\") - vector(\"hombre\") + vector(\"mujer\") â‰ˆ vector(\"reina\")\n",
    "```\n",
    "\n",
    "Esta propiedad demuestra que los embeddings capturan relaciones semÃ¡nticas complejas.\n",
    "\n",
    "## 2.2 CÃ³mo Funcionan los Modelos de Embeddings\n",
    "\n",
    "Los modelos de embeddings modernos se basan en arquitecturas **Transformer** (como BERT) entrenados con tÃ©cnicas de **aprendizaje contrastivo**:\n",
    "\n",
    "1. **Pares positivos**: Textos relacionados (pregunta-respuesta, parÃ¡frasis) deben tener embeddings similares\n",
    "2. **Pares negativos**: Textos no relacionados deben tener embeddings diferentes\n",
    "3. **Entrenamiento**: El modelo aprende a maximizar la similitud de pares positivos y minimizar la de negativos\n",
    "\n",
    "### Tipos de Modelos de Embeddings\n",
    "\n",
    "| Tipo | DescripciÃ³n | Ejemplo |\n",
    "|------|-------------|----------|\n",
    "| **Bi-encoder** | Codifica query y documento independientemente. RÃ¡pido para bÃºsqueda. | Sentence-BERT, BGE |\n",
    "| **Cross-encoder** | Procesa query y documento juntos. MÃ¡s preciso pero mÃ¡s lento. | ms-marco-MiniLM |\n",
    "| **Sparse embeddings** | Vectores dispersos, interpretables. | SPLADE |\n",
    "| **Dense embeddings** | Vectores densos, capturan semÃ¡ntica profunda. | E5, BGE-M3 |\n",
    "\n",
    "## 2.3 MÃ©tricas de Similitud\n",
    "\n",
    "Para comparar embeddings usamos mÃ©tricas de distancia o similitud:\n",
    "\n",
    "### Similitud Coseno (mÃ¡s comÃºn)\n",
    "\n",
    "Mide el Ã¡ngulo entre dos vectores, ignorando su magnitud:\n",
    "\n",
    "$$\\text{cosine\\_similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|}$$\n",
    "\n",
    "- **Valor 1**: Vectores idÃ©nticos en direcciÃ³n (muy similares)\n",
    "- **Valor 0**: Vectores perpendiculares (sin relaciÃ³n)\n",
    "- **Valor -1**: Vectores opuestos (significados opuestos)\n",
    "\n",
    "### Distancia Euclidiana\n",
    "\n",
    "Mide la distancia directa entre dos puntos:\n",
    "\n",
    "$$\\text{euclidean\\_distance}(A, B) = \\sqrt{\\sum_{i=1}^{n}(A_i - B_i)^2}$$\n",
    "\n",
    "### Producto Punto (Dot Product)\n",
    "\n",
    "Similar al coseno pero considera la magnitud:\n",
    "\n",
    "$$\\text{dot\\_product}(A, B) = \\sum_{i=1}^{n} A_i \\times B_i$$\n",
    "\n",
    "**RecomendaciÃ³n**: Para RAG, usar **similitud coseno** o **producto punto con vectores normalizados**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 PrÃ¡ctica: Trabajando con Embeddings\n",
    "\n",
    "Vamos a explorar cÃ³mo funcionan los embeddings en la prÃ¡ctica. Usaremos `sentence-transformers`, una biblioteca que facilita el uso de modelos de embeddings de Ãºltima generaciÃ³n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar un modelo de embeddings multilingÃ¼e\n",
    "# Este modelo funciona bien con espaÃ±ol y es relativamente ligero\n",
    "print(\"Cargando modelo de embeddings...\")\n",
    "embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# InformaciÃ³n del modelo\n",
    "dimension = embedding_model.get_sentence_embedding_dimension()\n",
    "print(f\"âœ… Modelo cargado correctamente\")\n",
    "print(f\"ğŸ“ DimensiÃ³n de los embeddings: {dimension}\")\n",
    "print(f\"ğŸ“ Esto significa que cada texto se convierte en un vector de {dimension} nÃºmeros\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 1: Generando Embeddings\n",
    "\n",
    "Veamos cÃ³mo convertir textos en vectores y explorar sus propiedades:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Textos de ejemplo sobre diferentes temas\n",
    "textos = [\n",
    "    \"El gato duerme plÃ¡cidamente en el sofÃ¡\",           # Tema: animales/hogar\n",
    "    \"El felino descansa sobre el sillÃ³n de la sala\",    # ParÃ¡frasis del anterior\n",
    "    \"Las proteÃ­nas chaperonas ayudan al plegamiento\",   # Tema: biologÃ­a molecular\n",
    "    \"Hsp70 asiste en el correcto doblez de proteÃ­nas\",  # ParÃ¡frasis del anterior\n",
    "    \"Python es un lenguaje de programaciÃ³n versÃ¡til\",   # Tema: programaciÃ³n\n",
    "]\n",
    "\n",
    "# Generar embeddings para todos los textos\n",
    "embeddings = embedding_model.encode(textos)\n",
    "\n",
    "# Explorar la estructura de los embeddings\n",
    "print(\"Estructura de los embeddings generados:\")\n",
    "print(f\"  - Tipo: {type(embeddings)}\")\n",
    "print(f\"  - Shape: {embeddings.shape}\")\n",
    "print(f\"  - Tenemos {embeddings.shape[0]} textos, cada uno con {embeddings.shape[1]} dimensiones\")\n",
    "print(f\"\\nPrimeras 10 dimensiones del primer embedding:\")\n",
    "print(f\"  {embeddings[0][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 2: Matriz de Similitud\n",
    "\n",
    "Ahora calculemos la similitud coseno entre todos los pares de textos. Esto nos mostrarÃ¡ quÃ© textos son semÃ¡nticamente similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la matriz de similitud coseno\n",
    "similarity_matrix = cosine_similarity(embeddings)\n",
    "\n",
    "# Crear una visualizaciÃ³n clara\n",
    "print(\"MATRIZ DE SIMILITUD COSENO\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nValores cercanos a 1.0 = textos muy similares\")\n",
    "print(\"Valores cercanos a 0.0 = textos no relacionados\\n\")\n",
    "\n",
    "# Mostrar los textos con Ã­ndices\n",
    "print(\"Textos:\")\n",
    "for i, texto in enumerate(textos):\n",
    "    print(f\"  T{i}: {texto}\")\n",
    "\n",
    "print(\"\\nMatriz de similitud:\")\n",
    "print(\"     \", end=\"\")\n",
    "for i in range(len(textos)):\n",
    "    print(f\"  T{i}  \", end=\"\")\n",
    "print()\n",
    "\n",
    "for i in range(len(textos)):\n",
    "    print(f\"T{i}  \", end=\"\")\n",
    "    for j in range(len(textos)):\n",
    "        sim = similarity_matrix[i][j]\n",
    "        # Colorear segÃºn similitud (usando sÃ­mbolos)\n",
    "        if i == j:\n",
    "            print(f\" 1.00 \", end=\"\")\n",
    "        elif sim > 0.7:\n",
    "            print(f\" {sim:.2f}*\", end=\"\")\n",
    "        else:\n",
    "            print(f\" {sim:.2f} \", end=\"\")\n",
    "    print()\n",
    "\n",
    "print(\"\\n* = Alta similitud (>0.7)\")\n",
    "print(\"\\nğŸ’¡ Observaciones:\")\n",
    "print(\"   - T0 y T1 (gato/felino) tienen alta similitud: son parÃ¡frasis\")\n",
    "print(\"   - T2 y T3 (proteÃ­nas/Hsp70) tienen alta similitud: mismo tema cientÃ­fico\")\n",
    "print(\"   - T4 (Python) tiene baja similitud con todos: tema diferente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 3: VisualizaciÃ³n en 2D\n",
    "\n",
    "Aunque los embeddings tienen muchas dimensiones (384 en este caso), podemos usar **PCA (AnÃ¡lisis de Componentes Principales)** para proyectarlos a 2D y visualizar cÃ³mo se agrupan los textos similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reducir a 2 dimensiones para visualizaciÃ³n\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Configurar el grÃ¡fico\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Colores por tema\n",
    "colores = ['#e74c3c', '#c0392b',  # Rojo: gato/felino\n",
    "           '#3498db', '#2980b9',  # Azul: proteÃ­nas\n",
    "           '#27ae60']             # Verde: programaciÃ³n\n",
    "\n",
    "# Dibujar puntos\n",
    "for i, (x, y) in enumerate(embeddings_2d):\n",
    "    plt.scatter(x, y, c=colores[i], s=200, alpha=0.7, edgecolors='white', linewidth=2)\n",
    "    \n",
    "    # Etiqueta abreviada\n",
    "    etiqueta = textos[i][:25] + \"...\" if len(textos[i]) > 25 else textos[i]\n",
    "    plt.annotate(f\"T{i}: {etiqueta}\", (x, y), \n",
    "                 fontsize=9, \n",
    "                 xytext=(10, 10), \n",
    "                 textcoords='offset points',\n",
    "                 bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.3))\n",
    "\n",
    "# Dibujar lÃ­neas entre textos similares\n",
    "plt.plot([embeddings_2d[0][0], embeddings_2d[1][0]], \n",
    "         [embeddings_2d[0][1], embeddings_2d[1][1]], \n",
    "         'r--', alpha=0.5, linewidth=2, label='ParÃ¡frasis (gato/felino)')\n",
    "plt.plot([embeddings_2d[2][0], embeddings_2d[3][0]], \n",
    "         [embeddings_2d[2][1], embeddings_2d[3][1]], \n",
    "         'b--', alpha=0.5, linewidth=2, label='ParÃ¡frasis (proteÃ­nas)')\n",
    "\n",
    "plt.title(\"VisualizaciÃ³n de Embeddings en 2D (PCA)\\nTextos similares aparecen cercanos\", fontsize=14)\n",
    "plt.xlabel(f\"Componente Principal 1 ({pca.explained_variance_ratio_[0]*100:.1f}% varianza)\")\n",
    "plt.ylabel(f\"Componente Principal 2 ({pca.explained_variance_ratio_[1]*100:.1f}% varianza)\")\n",
    "plt.legend(loc='best')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"ğŸ“Š El grÃ¡fico muestra cÃ³mo los textos con significados similares\")\n",
    "print(\"   se agrupan en el espacio vectorial, incluso usando diferentes palabras.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ejemplo 4: BÃºsqueda SemÃ¡ntica Simple\n",
    "\n",
    "Implementemos una bÃºsqueda semÃ¡ntica bÃ¡sica para entender cÃ³mo RAG encuentra documentos relevantes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def busqueda_semantica(consulta, documentos, modelo, top_k=3):\n",
    "    \"\"\"\n",
    "    Realiza una bÃºsqueda semÃ¡ntica simple.\n",
    "    \n",
    "    Args:\n",
    "        consulta: La pregunta o bÃºsqueda del usuario\n",
    "        documentos: Lista de documentos donde buscar\n",
    "        modelo: Modelo de embeddings\n",
    "        top_k: NÃºmero de resultados a devolver\n",
    "    \n",
    "    Returns:\n",
    "        Lista de tuplas (documento, score) ordenada por relevancia\n",
    "    \"\"\"\n",
    "    # Generar embedding de la consulta\n",
    "    embedding_consulta = modelo.encode([consulta])\n",
    "    \n",
    "    # Generar embeddings de los documentos\n",
    "    embeddings_docs = modelo.encode(documentos)\n",
    "    \n",
    "    # Calcular similitud coseno\n",
    "    similitudes = cosine_similarity(embedding_consulta, embeddings_docs)[0]\n",
    "    \n",
    "    # Ordenar por similitud descendente\n",
    "    indices_ordenados = np.argsort(similitudes)[::-1][:top_k]\n",
    "    \n",
    "    # Devolver resultados\n",
    "    resultados = [(documentos[i], similitudes[i]) for i in indices_ordenados]\n",
    "    return resultados\n",
    "\n",
    "\n",
    "# Base de conocimiento de ejemplo (simulando documentos cientÃ­ficos)\n",
    "base_conocimiento = [\n",
    "    \"Las proteÃ­nas chaperonas son molÃ©culas que asisten en el plegamiento correcto de otras proteÃ­nas.\",\n",
    "    \"Hsp70 reconoce secuencias hidrofÃ³bicas de 7-8 aminoÃ¡cidos en proteÃ­nas desplegadas.\",\n",
    "    \"El mal plegamiento de proteÃ­nas estÃ¡ asociado con enfermedades neurodegenerativas como Alzheimer.\",\n",
    "    \"CRISPR-Cas9 permite editar secuencias de ADN con alta precisiÃ³n.\",\n",
    "    \"La PCR amplifica secuencias especÃ­ficas de ADN para su anÃ¡lisis.\",\n",
    "    \"Los ribosomas son los orgÃ¡nulos encargados de sintetizar proteÃ­nas.\",\n",
    "    \"Hsp90 es esencial para la maduraciÃ³n de receptores de hormonas esteroides.\",\n",
    "    \"Las enfermedades por priones son causadas por proteÃ­nas mal plegadas infecciosas.\",\n",
    "]\n",
    "\n",
    "# Realizar una bÃºsqueda\n",
    "consulta = \"Â¿QuÃ© proteÃ­nas ayudan a otras proteÃ­nas a plegarse correctamente?\"\n",
    "\n",
    "print(f\"ğŸ” Consulta: \\\"{consulta}\\\"\")\n",
    "print(\"\\nğŸ“„ Resultados de la bÃºsqueda semÃ¡ntica:\\n\")\n",
    "\n",
    "resultados = busqueda_semantica(consulta, base_conocimiento, embedding_model, top_k=3)\n",
    "\n",
    "for i, (doc, score) in enumerate(resultados, 1):\n",
    "    print(f\"{i}. [Similitud: {score:.4f}]\")\n",
    "    print(f\"   {doc}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Modelos de Embeddings Recomendados\n",
    "\n",
    "La elecciÃ³n del modelo de embeddings es crucial para el rendimiento de RAG. AquÃ­ una comparativa detallada:\n",
    "\n",
    "### Modelos MultilingÃ¼es (Recomendados para espaÃ±ol)\n",
    "\n",
    "| Modelo | DimensiÃ³n | Contexto | CaracterÃ­sticas | Caso de Uso |\n",
    "|--------|-----------|----------|-----------------|-------------|\n",
    "| **BAAI/bge-m3** | 1024 | 8192 tokens | Estado del arte, soporta dense + sparse | ProducciÃ³n, mÃ¡xima calidad |\n",
    "| **intfloat/multilingual-e5-large** | 1024 | 512 tokens | Muy buen rendimiento en espaÃ±ol | RAG multilingÃ¼e |\n",
    "| **sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2** | 384 | 128 tokens | Ligero y rÃ¡pido | Prototipos, recursos limitados |\n",
    "\n",
    "### Modelos EspecÃ­ficos para EspaÃ±ol\n",
    "\n",
    "| Modelo | DimensiÃ³n | CaracterÃ­sticas |\n",
    "|--------|-----------|----------------|\n",
    "| **jinaai/jina-embeddings-v2-base-es** | 768 | Optimizado para espaÃ±ol |\n",
    "| **hiiamsid/sentence_similarity_spanish_es** | 768 | Entrenado especÃ­ficamente en espaÃ±ol |\n",
    "\n",
    "### Criterios de SelecciÃ³n\n",
    "\n",
    "1. **Idioma**: Priorizar modelos multilingÃ¼es o especÃ­ficos para espaÃ±ol\n",
    "2. **DimensiÃ³n**: Mayor dimensiÃ³n = mÃ¡s capacidad semÃ¡ntica pero mÃ¡s memoria\n",
    "3. **Contexto mÃ¡ximo**: Importante si los chunks son largos\n",
    "4. **Velocidad**: Considerar latencia en producciÃ³n\n",
    "5. **Licencia**: Verificar compatibilidad con uso comercial/acadÃ©mico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"3-chunking\"></a>\n",
    "# 3. Chunking: DivisiÃ³n Inteligente de Documentos\n",
    "\n",
    "## 3.1 Â¿Por quÃ© es Necesario el Chunking?\n",
    "\n",
    "El **chunking** (fragmentaciÃ³n) es el proceso de dividir documentos largos en fragmentos mÃ¡s pequeÃ±os. Es necesario por varias razones:\n",
    "\n",
    "### Limitaciones TÃ©cnicas\n",
    "\n",
    "1. **LÃ­mite de tokens del modelo de embeddings**: La mayorÃ­a de modelos tienen un mÃ¡ximo de 512-8192 tokens\n",
    "2. **Ventana de contexto del LLM**: Aunque algunos LLMs aceptan 128K tokens, el rendimiento decrece con contextos muy largos\n",
    "3. **Memoria y velocidad**: Procesar documentos completos es ineficiente\n",
    "\n",
    "### Beneficios para la Calidad\n",
    "\n",
    "1. **PrecisiÃ³n en la bÃºsqueda**: Fragmentos pequeÃ±os permiten recuperar informaciÃ³n mÃ¡s especÃ­fica\n",
    "2. **Relevancia del contexto**: El LLM recibe solo la informaciÃ³n pertinente, no todo el documento\n",
    "3. **Mejor citaciÃ³n**: Se puede indicar exactamente quÃ© parte del documento se usÃ³\n",
    "\n",
    "## 3.2 El Dilema del TamaÃ±o del Chunk\n",
    "\n",
    "Elegir el tamaÃ±o correcto es un balance:\n",
    "\n",
    "```\n",
    "Chunks MUY PEQUEÃ‘OS              Chunks MUY GRANDES\n",
    "        â—„â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º\n",
    "        \n",
    "âœ“ Alta precisiÃ³n en bÃºsqueda    âœ— Baja precisiÃ³n en bÃºsqueda\n",
    "âœ— PÃ©rdida de contexto           âœ“ Contexto completo\n",
    "âœ— FragmentaciÃ³n de ideas        âœ“ Ideas completas\n",
    "âœ“ Menos tokens por consulta     âœ— MÃ¡s tokens por consulta\n",
    "```\n",
    "\n",
    "### Recomendaciones Generales\n",
    "\n",
    "| Caso de Uso | TamaÃ±o Recomendado | Overlap |\n",
    "|-------------|-------------------|----------|\n",
    "| Documentos tÃ©cnicos/cientÃ­ficos | 500-1000 tokens | 10-20% |\n",
    "| FAQs y documentos cortos | 200-500 tokens | 5-10% |\n",
    "| CÃ³digo fuente | 100-300 tokens | 0-10% |\n",
    "| Conversaciones/chat | 300-500 tokens | 20-30% |\n",
    "\n",
    "## 3.3 Estrategias de Chunking\n",
    "\n",
    "### 1. Fixed Size Chunking (TamaÃ±o Fijo)\n",
    "\n",
    "La estrategia mÃ¡s simple: dividir por nÃºmero de caracteres o tokens.\n",
    "\n",
    "**Ventajas:**\n",
    "- Simple de implementar\n",
    "- Chunks de tamaÃ±o predecible\n",
    "\n",
    "**Desventajas:**\n",
    "- Puede cortar a mitad de oraciÃ³n o pÃ¡rrafo\n",
    "- No respeta la estructura del documento\n",
    "\n",
    "### 2. Recursive Character Splitting (Recomendado)\n",
    "\n",
    "Divide usando una jerarquÃ­a de separadores: primero intenta dividir por pÃ¡rrafos, luego por oraciones, luego por palabras.\n",
    "\n",
    "**Ventajas:**\n",
    "- Respeta mejor las unidades semÃ¡nticas\n",
    "- Balance entre simplicidad y calidad\n",
    "\n",
    "**Desventajas:**\n",
    "- TamaÃ±os de chunks menos uniformes\n",
    "\n",
    "### 3. Semantic Chunking\n",
    "\n",
    "Usa embeddings para detectar cambios de tema y dividir en esos puntos.\n",
    "\n",
    "**Ventajas:**\n",
    "- Chunks coherentes temÃ¡ticamente\n",
    "- Ideal para documentos con mÃºltiples temas\n",
    "\n",
    "**Desventajas:**\n",
    "- MÃ¡s costoso computacionalmente\n",
    "- Requiere calibraciÃ³n\n",
    "\n",
    "### 4. Document Structure Chunking\n",
    "\n",
    "Respeta la estructura del documento (tÃ­tulos, secciones, pÃ¡rrafos).\n",
    "\n",
    "**Ventajas:**\n",
    "- Preserva la organizaciÃ³n original\n",
    "- Mantiene metadatos de secciÃ³n\n",
    "\n",
    "**Desventajas:**\n",
    "- Requiere documentos bien estructurados\n",
    "- TamaÃ±os muy variables\n",
    "\n",
    "## 3.4 El Concepto de Overlap (Solapamiento)\n",
    "\n",
    "El **overlap** es la cantidad de texto que se repite entre chunks consecutivos. Es crucial para no perder contexto en los bordes:\n",
    "\n",
    "```\n",
    "Sin overlap:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚    Chunk 1     â”‚ â”‚    Chunk 2     â”‚ â”‚    Chunk 3     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                  â–²                  â–²\n",
    "            InformaciÃ³n perdida en los bordes\n",
    "\n",
    "Con overlap del 20%:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚      Chunk 1       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "     â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "     â”‚      Chunk 2      â”‚\n",
    "     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "          â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "          â”‚     Chunk 3      â”‚\n",
    "          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "El overlap asegura que la informaciÃ³n que aparece en la frontera entre chunks no se pierda."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 PrÃ¡ctica: Implementando Diferentes Estrategias de Chunking\n",
    "\n",
    "Vamos a implementar y comparar diferentes estrategias de chunking con un documento cientÃ­fico de ejemplo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documento cientÃ­fico de ejemplo\n",
    "documento_cientifico = \"\"\"\n",
    "# ProteÃ­nas Chaperonas: Guardianes del Plegamiento Celular\n",
    "\n",
    "## IntroducciÃ³n\n",
    "\n",
    "Las proteÃ­nas chaperonas representan una clase fundamental de molÃ©culas que asisten en el \n",
    "plegamiento correcto de otras proteÃ­nas. Descubiertas inicialmente como proteÃ­nas de choque \n",
    "tÃ©rmico (Heat Shock Proteins, HSPs), hoy sabemos que desempeÃ±an funciones esenciales en \n",
    "condiciones normales y de estrÃ©s celular.\n",
    "\n",
    "El plegamiento de proteÃ­nas es un proceso crÃ­tico: una cadena polipeptÃ­dica debe adoptar \n",
    "una estructura tridimensional especÃ­fica para ser funcional. Errores en este proceso pueden \n",
    "llevar a enfermedades graves, incluyendo Alzheimer, Parkinson y otras patologÃ­as \n",
    "neurodegenerativas.\n",
    "\n",
    "## Mecanismo de AcciÃ³n\n",
    "\n",
    "Las chaperonas funcionan reconociendo regiones hidrofÃ³bicas que quedan expuestas durante \n",
    "el plegamiento. En una proteÃ­na correctamente plegada, estas regiones estÃ¡n ocultas en el \n",
    "interior; su exposiciÃ³n indica un estado de plegamiento incorrecto o incompleto.\n",
    "\n",
    "El mecanismo general implica:\n",
    "\n",
    "1. Reconocimiento de la proteÃ­na sustrato desplegada\n",
    "2. UniÃ³n a travÃ©s de dominios especÃ­ficos de uniÃ³n a sustrato\n",
    "3. Ciclos de uniÃ³n y liberaciÃ³n dependientes de ATP\n",
    "4. FacilitaciÃ³n del plegamiento correcto o direccionamiento a degradaciÃ³n\n",
    "\n",
    "## Familia Hsp70\n",
    "\n",
    "La familia Hsp70 es una de las mÃ¡s conservadas evolutivamente y mejor caracterizadas. \n",
    "Sus miembros se encuentran en prÃ¡cticamente todos los organismos, desde bacterias hasta \n",
    "humanos.\n",
    "\n",
    "CaracterÃ­sticas principales de Hsp70:\n",
    "\n",
    "- Reconoce segmentos hidrofÃ³bicos de 7-8 aminoÃ¡cidos\n",
    "- Requiere co-chaperonas como Hsp40 (DnaJ) para su actividad Ã³ptima\n",
    "- Funciona mediante un ciclo ATPasa regulado\n",
    "- Participa en mÃºltiples procesos: plegamiento, translocaciÃ³n, desensamblaje de complejos\n",
    "\n",
    "## Familia Hsp90\n",
    "\n",
    "Hsp90 es esencial para la maduraciÃ³n de un conjunto especÃ­fico de proteÃ­nas cliente, \n",
    "muchas de las cuales son factores de seÃ±alizaciÃ³n celular.\n",
    "\n",
    "Entre sus clientes se incluyen:\n",
    "\n",
    "- Receptores de hormonas esteroides\n",
    "- Quinasas de seÃ±alizaciÃ³n\n",
    "- Factores de transcripciÃ³n\n",
    "- ProteÃ­nas del ciclo celular\n",
    "\n",
    "## Relevancia ClÃ­nica\n",
    "\n",
    "Las chaperonas tienen una relevancia clÃ­nica significativa en mÃºltiples contextos:\n",
    "\n",
    "### Enfermedades Neurodegenerativas\n",
    "\n",
    "El mal plegamiento y agregaciÃ³n de proteÃ­nas es la causa subyacente de numerosas \n",
    "enfermedades neurodegenerativas:\n",
    "\n",
    "- Alzheimer: agregados de beta-amiloide y tau hiperfosforilada\n",
    "- Parkinson: agregados de alfa-sinucleÃ­na (cuerpos de Lewy)\n",
    "- Huntington: agregados de huntingtina con expansiÃ³n de poliglutamina\n",
    "- Enfermedades por priones: agregados de PrPSc\n",
    "\n",
    "### CÃ¡ncer\n",
    "\n",
    "Las cÃ©lulas tumorales frecuentemente muestran dependencia de chaperonas, particularmente \n",
    "Hsp90. Esta \"adicciÃ³n a chaperonas\" ha llevado al desarrollo de inhibidores de Hsp90 \n",
    "como estrategia terapÃ©utica anticancerÃ­gena.\n",
    "\n",
    "## Conclusiones\n",
    "\n",
    "Las proteÃ­nas chaperonas son componentes esenciales de la maquinaria celular de control \n",
    "de calidad proteica. Su estudio continÃºa revelando nuevos mecanismos y posibilidades \n",
    "terapÃ©uticas para enfermedades asociadas al mal plegamiento de proteÃ­nas.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"ğŸ“„ Documento cargado\")\n",
    "print(f\"   - Longitud total: {len(documento_cientifico)} caracteres\")\n",
    "print(f\"   - Palabras aproximadas: {len(documento_cientifico.split())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    MarkdownHeaderTextSplitter,\n",
    ")\n",
    "\n",
    "# ============================================================\n",
    "# ESTRATEGIA 1: Character Text Splitter (DivisiÃ³n simple)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESTRATEGIA 1: Character Text Splitter\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDivide por un separador especÃ­fico (ej: salto de lÃ­nea)\")\n",
    "print(\"Simple pero puede cortar oraciones a mitad.\\n\")\n",
    "\n",
    "char_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",      # Dividir por saltos de lÃ­nea\n",
    "    chunk_size=500,      # MÃ¡ximo 500 caracteres por chunk\n",
    "    chunk_overlap=50,    # 50 caracteres de overlap\n",
    "    length_function=len,\n",
    ")\n",
    "\n",
    "chunks_char = char_splitter.split_text(documento_cientifico)\n",
    "\n",
    "print(f\"NÃºmero de chunks generados: {len(chunks_char)}\")\n",
    "print(f\"\\nEjemplo - Chunk 1 ({len(chunks_char[0])} chars):\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks_char[0][:300] + \"...\" if len(chunks_char[0]) > 300 else chunks_char[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIA 2: Recursive Character Text Splitter (RECOMENDADA)\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESTRATEGIA 2: Recursive Character Text Splitter (RECOMENDADA)\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nIntenta dividir jerÃ¡rquicamente: pÃ¡rrafos â†’ oraciones â†’ palabras\")\n",
    "print(\"Mejor preservaciÃ³n de unidades semÃ¡nticas.\\n\")\n",
    "\n",
    "recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,  # 20% de overlap\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \", \", \" \", \"\"]  # JerarquÃ­a de separadores\n",
    ")\n",
    "\n",
    "chunks_recursive = recursive_splitter.split_text(documento_cientifico)\n",
    "\n",
    "print(f\"NÃºmero de chunks generados: {len(chunks_recursive)}\")\n",
    "print(f\"\\nTamaÃ±os de los primeros 5 chunks:\")\n",
    "for i, chunk in enumerate(chunks_recursive[:5]):\n",
    "    print(f\"  Chunk {i+1}: {len(chunk)} caracteres\")\n",
    "\n",
    "print(f\"\\nEjemplo - Chunk 3:\")\n",
    "print(\"-\" * 50)\n",
    "print(chunks_recursive[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ESTRATEGIA 3: Markdown Header Text Splitter\n",
    "# ============================================================\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"ESTRATEGIA 3: Markdown Header Text Splitter\")\n",
    "print(\"=\"*70)\n",
    "print(\"\\nDivide respetando la estructura de encabezados Markdown.\")\n",
    "print(\"Preserva metadatos de secciÃ³n para mejor contexto.\\n\")\n",
    "\n",
    "# Definir los headers que queremos usar para dividir\n",
    "headers_to_split = [\n",
    "    (\"#\", \"titulo\"),\n",
    "    (\"##\", \"seccion\"),\n",
    "    (\"###\", \"subseccion\"),\n",
    "]\n",
    "\n",
    "markdown_splitter = MarkdownHeaderTextSplitter(\n",
    "    headers_to_split_on=headers_to_split\n",
    ")\n",
    "\n",
    "chunks_markdown = markdown_splitter.split_text(documento_cientifico)\n",
    "\n",
    "print(f\"NÃºmero de chunks generados: {len(chunks_markdown)}\")\n",
    "print(f\"\\nChunks con sus metadatos de secciÃ³n:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, chunk in enumerate(chunks_markdown[:4]):\n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ğŸ“ Metadatos: {chunk.metadata}\")\n",
    "    contenido_preview = chunk.page_content[:150].replace('\\n', ' ')\n",
    "    print(f\"  ğŸ“ Contenido: {contenido_preview}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Combinando Estrategias: Pipeline de Chunking Ã“ptimo\n",
    "\n",
    "En la prÃ¡ctica, la mejor aproximaciÃ³n es combinar estrategias: primero dividir por estructura del documento, luego aplicar chunking recursivo a secciones grandes.\n",
    "\n",
    "AdemÃ¡s, es importante **enriquecer cada chunk con metadatos** que ayuden en la recuperaciÃ³n y citaciÃ³n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunking_optimizado(documento, chunk_size=500, chunk_overlap=100):\n",
    "    \"\"\"\n",
    "    Pipeline de chunking optimizado que combina estructura y tamaÃ±o.\n",
    "    \n",
    "    1. Primero divide por estructura (headers markdown)\n",
    "    2. Luego aplica chunking recursivo a secciones grandes\n",
    "    3. Preserva metadatos de origen\n",
    "    \"\"\"\n",
    "    from langchain.schema import Document\n",
    "    \n",
    "    # Paso 1: Dividir por estructura\n",
    "    headers = [(\"#\", \"titulo\"), (\"##\", \"seccion\"), (\"###\", \"subseccion\")]\n",
    "    md_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers)\n",
    "    chunks_estructura = md_splitter.split_text(documento)\n",
    "    \n",
    "    # Paso 2: Aplicar chunking recursivo a secciones grandes\n",
    "    recursive_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "    )\n",
    "    \n",
    "    chunks_finales = []\n",
    "    \n",
    "    for chunk in chunks_estructura:\n",
    "        contenido = chunk.page_content\n",
    "        metadata = chunk.metadata.copy()\n",
    "        \n",
    "        if len(contenido) > chunk_size:\n",
    "            # SecciÃ³n grande: dividir mÃ¡s\n",
    "            sub_chunks = recursive_splitter.split_text(contenido)\n",
    "            for i, sub_chunk in enumerate(sub_chunks):\n",
    "                new_metadata = metadata.copy()\n",
    "                new_metadata['parte'] = i + 1\n",
    "                new_metadata['total_partes'] = len(sub_chunks)\n",
    "                chunks_finales.append(Document(\n",
    "                    page_content=sub_chunk,\n",
    "                    metadata=new_metadata\n",
    "                ))\n",
    "        else:\n",
    "            # SecciÃ³n pequeÃ±a: mantener como estÃ¡\n",
    "            metadata['parte'] = 1\n",
    "            metadata['total_partes'] = 1\n",
    "            chunks_finales.append(Document(\n",
    "                page_content=contenido,\n",
    "                metadata=metadata\n",
    "            ))\n",
    "    \n",
    "    return chunks_finales\n",
    "\n",
    "\n",
    "# Aplicar el chunking optimizado\n",
    "chunks_optimizados = chunking_optimizado(documento_cientifico, chunk_size=400, chunk_overlap=80)\n",
    "\n",
    "print(\"CHUNKING OPTIMIZADO: Resultados\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"\\nTotal de chunks generados: {len(chunks_optimizados)}\")\n",
    "print(f\"\\nDetalle de cada chunk:\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for i, chunk in enumerate(chunks_optimizados):\n",
    "    seccion = chunk.metadata.get('seccion', 'Principal')\n",
    "    parte = chunk.metadata.get('parte', 1)\n",
    "    total = chunk.metadata.get('total_partes', 1)\n",
    "    \n",
    "    print(f\"\\nChunk {i+1}:\")\n",
    "    print(f\"  ğŸ“ SecciÃ³n: {seccion}\")\n",
    "    print(f\"  ğŸ“„ Parte: {parte}/{total}\")\n",
    "    print(f\"  ğŸ“ Longitud: {len(chunk.page_content)} caracteres\")\n",
    "    print(f\"  ğŸ“ Preview: {chunk.page_content[:80].replace(chr(10), ' ')}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Mejores PrÃ¡cticas de Chunking\n",
    "\n",
    "### Lista de VerificaciÃ³n para Chunking de Calidad\n",
    "\n",
    "1. **Preservar unidades semÃ¡nticas**\n",
    "   - No cortar a mitad de oraciÃ³n\n",
    "   - Preferir cortes en lÃ­mites de pÃ¡rrafo\n",
    "   - Respetar estructura del documento cuando sea posible\n",
    "\n",
    "2. **Configurar overlap adecuado**\n",
    "   - 10-20% del tamaÃ±o del chunk es lo recomendado\n",
    "   - MÃ¡s overlap para documentos con ideas que fluyen entre pÃ¡rrafos\n",
    "   - Menos overlap para documentos con secciones independientes\n",
    "\n",
    "3. **Enriquecer con metadatos**\n",
    "   - Fuente del documento (archivo, URL)\n",
    "   - NÃºmero de pÃ¡gina o secciÃ³n\n",
    "   - Fecha de creaciÃ³n/actualizaciÃ³n\n",
    "   - Autor o departamento\n",
    "   - CategorÃ­a o tags\n",
    "\n",
    "4. **Limpiar el texto**\n",
    "   - Eliminar headers/footers repetitivos\n",
    "   - Corregir errores de OCR si aplica\n",
    "   - Normalizar espacios y saltos de lÃ­nea\n",
    "   - Manejar tablas y figuras apropiadamente\n",
    "\n",
    "5. **Adaptar al dominio**\n",
    "   - Documentos cientÃ­ficos: chunks mÃ¡s grandes para mantener contexto\n",
    "   - FAQs: chunks mÃ¡s pequeÃ±os, uno por pregunta\n",
    "   - CÃ³digo: respetar funciones/clases como unidades\n",
    "\n",
    "### ParÃ¡metros Recomendados por Tipo de Documento\n",
    "\n",
    "| Tipo de Documento | chunk_size | chunk_overlap | Estrategia |\n",
    "|-------------------|------------|---------------|------------|\n",
    "| Papers cientÃ­ficos | 800-1200 | 150-200 | Recursive + Structure |\n",
    "| DocumentaciÃ³n tÃ©cnica | 500-800 | 100-150 | Recursive |\n",
    "| Normativas/Legal | 600-1000 | 100-200 | Structure-aware |\n",
    "| FAQs | 200-400 | 50 | Por pregunta |\n",
    "| CÃ³digo fuente | 200-500 | 50-100 | Por funciÃ³n/clase |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"4-vectorstores\"></a>\n",
    "# 4. Vector Stores: Almacenamiento y BÃºsqueda SemÃ¡ntica\n",
    "\n",
    "## 4.1 Â¿QuÃ© es un Vector Store?\n",
    "\n",
    "Un **Vector Store** (o base de datos vectorial) es un sistema especializado en almacenar y buscar vectores de alta dimensiÃ³n. A diferencia de las bases de datos tradicionales que buscan por coincidencia exacta, los vector stores permiten **bÃºsqueda por similitud**: encontrar los vectores mÃ¡s parecidos a uno dado.\n",
    "\n",
    "### Â¿Por quÃ© no usar una base de datos tradicional?\n",
    "\n",
    "Las bases de datos relacionales (PostgreSQL, MySQL) o documentales (MongoDB) no estÃ¡n optimizadas para:\n",
    "\n",
    "1. **Almacenar vectores de alta dimensiÃ³n** (384-4096 dimensiones)\n",
    "2. **BÃºsqueda por similitud** en tiempo sublineal\n",
    "3. **Escalabilidad** con millones de vectores\n",
    "\n",
    "Los vector stores utilizan estructuras de datos especializadas como **HNSW (Hierarchical Navigable Small World)** o **IVF (Inverted File Index)** que permiten bÃºsquedas aproximadas muy rÃ¡pidas.\n",
    "\n",
    "## 4.2 Algoritmos de IndexaciÃ³n\n",
    "\n",
    "### BÃºsqueda Exacta (Brute Force)\n",
    "\n",
    "Compara el vector de consulta con **todos** los vectores almacenados.\n",
    "\n",
    "- **Complejidad**: O(n) donde n = nÃºmero de vectores\n",
    "- **Ventaja**: 100% de precisiÃ³n (recall perfecto)\n",
    "- **Desventaja**: Muy lento para grandes colecciones\n",
    "- **Uso**: Colecciones pequeÃ±as (<10,000 vectores)\n",
    "\n",
    "### HNSW (Hierarchical Navigable Small World)\n",
    "\n",
    "Construye un grafo multicapa donde cada nodo estÃ¡ conectado a sus vecinos mÃ¡s cercanos.\n",
    "\n",
    "```\n",
    "Capa 3:    â—‹â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—‹  (pocos nodos, conexiones largas)\n",
    "           â”‚               â”‚\n",
    "Capa 2:    â—‹â”€â”€â”€â—‹â”€â”€â”€â”€â”€â”€â”€â—‹â”€â”€â”€â—‹  (mÃ¡s nodos)\n",
    "           â”‚   â”‚       â”‚   â”‚\n",
    "Capa 1:    â—‹â”€â—‹â”€â—‹â”€â—‹â”€â”€â”€â—‹â”€â—‹â”€â—‹â”€â—‹  (mÃ¡s nodos aÃºn)\n",
    "           â”‚ â”‚ â”‚ â”‚   â”‚ â”‚ â”‚ â”‚\n",
    "Capa 0:    â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹â—‹  (todos los nodos)\n",
    "```\n",
    "\n",
    "- **Complejidad**: O(log n) para bÃºsqueda\n",
    "- **Ventaja**: Muy rÃ¡pido, alta precisiÃ³n (>95%)\n",
    "- **Desventaja**: Mayor uso de memoria\n",
    "- **Uso**: La mayorÃ­a de aplicaciones en producciÃ³n\n",
    "\n",
    "### IVF (Inverted File Index)\n",
    "\n",
    "Divide el espacio vectorial en clusters (regiones). Para buscar, primero identifica los clusters relevantes y luego busca solo dentro de ellos.\n",
    "\n",
    "- **Complejidad**: O(n/k) donde k = nÃºmero de clusters visitados\n",
    "- **Ventaja**: Buen balance velocidad/memoria\n",
    "- **Desventaja**: Requiere entrenamiento previo\n",
    "- **Uso**: Colecciones muy grandes (>1M vectores)\n",
    "\n",
    "## 4.3 Comparativa de Soluciones\n",
    "\n",
    "| SoluciÃ³n | Licencia | Tipo | Mejor Para | CaracterÃ­sticas |\n",
    "|----------|----------|------|------------|----------------|\n",
    "| **ChromaDB** | Apache 2.0 | Embebido | Prototipos, desarrollo | Simple, Python-native, sin servidor |\n",
    "| **FAISS** | MIT | Biblioteca | Alto rendimiento | Facebook, muy optimizado, bajo nivel |\n",
    "| **Qdrant** | Apache 2.0 | Servidor | ProducciÃ³n | Filtros, REST API, escalable |\n",
    "| **Milvus** | Apache 2.0 | Servidor | Enterprise | Cloud-native, escalabilidad masiva |\n",
    "| **Weaviate** | BSD-3 | Servidor | BÃºsqueda hÃ­brida | GraphQL, mÃ³dulos ML integrados |\n",
    "| **Pinecone** | Propietario | Cloud | Managed service | Sin operaciones, serverless |\n",
    "| **pgvector** | PostgreSQL | ExtensiÃ³n | IntegraciÃ³n SQL | Usa PostgreSQL existente |\n",
    "\n",
    "### Recomendaciones por Caso de Uso\n",
    "\n",
    "- **Aprendizaje/Prototipos**: ChromaDB (simple, sin configuraciÃ³n)\n",
    "- **Proyecto pequeÃ±o/mediano**: Qdrant o ChromaDB persistente\n",
    "- **ProducciÃ³n con alto rendimiento**: Qdrant, Milvus o FAISS\n",
    "- **Ya tienes PostgreSQL**: pgvector\n",
    "- **Sin recursos para operar**: Pinecone (cloud managed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 PrÃ¡ctica: ImplementaciÃ³n con ChromaDB\n",
    "\n",
    "ChromaDB es ideal para aprender y prototipar porque:\n",
    "- Se ejecuta en memoria o con persistencia local\n",
    "- No requiere servidor externo\n",
    "- API simple e intuitiva\n",
    "- IntegraciÃ³n nativa con modelos de embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# ============================================================\n",
    "# Crear cliente de ChromaDB\n",
    "# ============================================================\n",
    "\n",
    "# OpciÃ³n 1: En memoria (se pierde al cerrar)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# OpciÃ³n 2: Persistente (descomentar para usar)\n",
    "# client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "\n",
    "print(\"âœ… Cliente ChromaDB creado\")\n",
    "\n",
    "# ============================================================\n",
    "# Configurar funciÃ³n de embeddings\n",
    "# ============================================================\n",
    "\n",
    "# Usar sentence-transformers (mismo modelo que usamos antes)\n",
    "embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
    ")\n",
    "\n",
    "print(\"âœ… FunciÃ³n de embeddings configurada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Crear una colecciÃ³n\n",
    "# ============================================================\n",
    "\n",
    "# Las colecciones son como \"tablas\" en bases de datos relacionales\n",
    "# Cada colecciÃ³n tiene su propio Ã­ndice y puede tener diferentes configuraciones\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"documentos_cientificos\",\n",
    "    embedding_function=embedding_fn,\n",
    "    metadata={\n",
    "        \"hnsw:space\": \"cosine\",  # MÃ©trica de distancia: cosine, l2, ip\n",
    "        \"description\": \"ColecciÃ³n de documentos cientÃ­ficos sobre biologÃ­a molecular\"\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"âœ… ColecciÃ³n '{collection.name}' creada/obtenida\")\n",
    "print(f\"   Documentos actuales: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# AÃ±adir documentos a la colecciÃ³n\n",
    "# ============================================================\n",
    "\n",
    "# Documentos de ejemplo (simulando chunks de un paper cientÃ­fico)\n",
    "documentos = [\n",
    "    \"Las proteÃ­nas chaperonas son molÃ©culas esenciales que asisten en el plegamiento correcto de otras proteÃ­nas mediante ciclos de uniÃ³n y liberaciÃ³n dependientes de ATP.\",\n",
    "    \"La familia Hsp70 reconoce secuencias hidrofÃ³bicas de 7-8 aminoÃ¡cidos en proteÃ­nas parcialmente desplegadas o mal plegadas.\",\n",
    "    \"Hsp90 es crucial para la maduraciÃ³n de receptores de hormonas esteroides, quinasas de seÃ±alizaciÃ³n y factores de transcripciÃ³n.\",\n",
    "    \"El mal plegamiento de proteÃ­nas estÃ¡ asociado con enfermedades neurodegenerativas como Alzheimer, Parkinson y enfermedades por priones.\",\n",
    "    \"CRISPR-Cas9 es una herramienta de ediciÃ³n genÃ©tica que permite modificar secuencias de ADN con alta precisiÃ³n usando un ARN guÃ­a.\",\n",
    "    \"La reacciÃ³n en cadena de la polimerasa (PCR) permite amplificar secuencias especÃ­ficas de ADN millones de veces para su anÃ¡lisis.\",\n",
    "    \"Los ribosomas son complejos macromoleculares responsables de la sÃ­ntesis de proteÃ­nas, traduciendo el ARN mensajero a cadenas polipeptÃ­dicas.\",\n",
    "    \"La espectroscopÃ­a de masas es una tÃ©cnica analÃ­tica que permite identificar proteÃ­nas y caracterizar sus modificaciones post-traduccionales.\",\n",
    "    \"Las co-chaperonas como Hsp40 (DnaJ) regulan la actividad ATPasa de Hsp70 y ayudan en el reconocimiento de sustratos.\",\n",
    "    \"Los inhibidores de Hsp90 son una estrategia terapÃ©utica prometedora en cÃ¡ncer debido a la dependencia de las cÃ©lulas tumorales en esta chaperona.\"\n",
    "]\n",
    "\n",
    "# Metadatos para cada documento\n",
    "metadatos = [\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"general\", \"fuente\": \"review_chaperonas.pdf\", \"pagina\": 1},\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"hsp70\", \"fuente\": \"review_chaperonas.pdf\", \"pagina\": 5},\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"hsp90\", \"fuente\": \"review_chaperonas.pdf\", \"pagina\": 8},\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"patologia\", \"fuente\": \"neurodegeneration.pdf\", \"pagina\": 12},\n",
    "    {\"tema\": \"genetica\", \"subtema\": \"crispr\", \"fuente\": \"gene_editing.pdf\", \"pagina\": 3},\n",
    "    {\"tema\": \"genetica\", \"subtema\": \"pcr\", \"fuente\": \"molecular_techniques.pdf\", \"pagina\": 7},\n",
    "    {\"tema\": \"biologia_celular\", \"subtema\": \"ribosomas\", \"fuente\": \"cell_biology.pdf\", \"pagina\": 45},\n",
    "    {\"tema\": \"proteomica\", \"subtema\": \"espectrometria\", \"fuente\": \"proteomics_handbook.pdf\", \"pagina\": 23},\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"cochaperonas\", \"fuente\": \"review_chaperonas.pdf\", \"pagina\": 6},\n",
    "    {\"tema\": \"chaperonas\", \"subtema\": \"terapeutica\", \"fuente\": \"cancer_therapy.pdf\", \"pagina\": 15},\n",
    "]\n",
    "\n",
    "# IDs Ãºnicos para cada documento\n",
    "ids = [f\"doc_{i:03d}\" for i in range(len(documentos))]\n",
    "\n",
    "# AÃ±adir a la colecciÃ³n\n",
    "collection.add(\n",
    "    documents=documentos,\n",
    "    metadatas=metadatos,\n",
    "    ids=ids\n",
    ")\n",
    "\n",
    "print(f\"âœ… {len(documentos)} documentos aÃ±adidos a la colecciÃ³n\")\n",
    "print(f\"   Total de documentos en la colecciÃ³n: {collection.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 BÃºsqueda SemÃ¡ntica con ChromaDB\n",
    "\n",
    "Ahora que tenemos documentos indexados, podemos realizar diferentes tipos de bÃºsqueda:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BÃºsqueda semÃ¡ntica bÃ¡sica\n",
    "# ============================================================\n",
    "\n",
    "consulta = \"Â¿QuÃ© molÃ©culas ayudan a las proteÃ­nas a plegarse correctamente?\"\n",
    "\n",
    "resultados = collection.query(\n",
    "    query_texts=[consulta],  # ChromaDB genera el embedding automÃ¡ticamente\n",
    "    n_results=3,              # Top 3 resultados\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]  # QuÃ© incluir en resultados\n",
    ")\n",
    "\n",
    "print(f\"ğŸ” BÃšSQUEDA SEMÃNTICA BÃSICA\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Consulta: \\\"{consulta}\\\"\")\n",
    "print(f\"\\nğŸ“„ Resultados (Top 3):\\n\")\n",
    "\n",
    "for i in range(len(resultados['documents'][0])):\n",
    "    doc = resultados['documents'][0][i]\n",
    "    meta = resultados['metadatas'][0][i]\n",
    "    dist = resultados['distances'][0][i]\n",
    "    similitud = 1 - dist  # Convertir distancia a similitud\n",
    "    \n",
    "    print(f\"{i+1}. [Similitud: {similitud:.4f}]\")\n",
    "    print(f\"   ğŸ“ Fuente: {meta['fuente']}, pÃ¡gina {meta['pagina']}\")\n",
    "    print(f\"   ğŸ·ï¸  Tema: {meta['tema']} > {meta['subtema']}\")\n",
    "    print(f\"   ğŸ“ {doc[:100]}...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BÃºsqueda con filtros de metadatos\n",
    "# ============================================================\n",
    "\n",
    "# Podemos filtrar por metadatos ANTES de la bÃºsqueda semÃ¡ntica\n",
    "# Esto es muy Ãºtil para acotar el dominio de bÃºsqueda\n",
    "\n",
    "consulta = \"mecanismo de acciÃ³n molecular\"\n",
    "\n",
    "print(f\"ğŸ” BÃšSQUEDA CON FILTROS DE METADATOS\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Consulta: \\\"{consulta}\\\"\")\n",
    "print(f\"Filtro: Solo documentos del tema 'chaperonas'\\n\")\n",
    "\n",
    "resultados_filtrados = collection.query(\n",
    "    query_texts=[consulta],\n",
    "    n_results=5,\n",
    "    where={\"tema\": \"chaperonas\"},  # Filtro de metadatos\n",
    "    include=[\"documents\", \"metadatas\", \"distances\"]\n",
    ")\n",
    "\n",
    "print(f\"ğŸ“„ Resultados (filtrados por tema='chaperonas'):\\n\")\n",
    "\n",
    "for i in range(len(resultados_filtrados['documents'][0])):\n",
    "    doc = resultados_filtrados['documents'][0][i]\n",
    "    meta = resultados_filtrados['metadatas'][0][i]\n",
    "    dist = resultados_filtrados['distances'][0][i]\n",
    "    \n",
    "    print(f\"{i+1}. [{meta['subtema']}] {doc[:80]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Filtros avanzados con operadores lÃ³gicos\n",
    "# ============================================================\n",
    "\n",
    "print(f\"ğŸ” FILTROS AVANZADOS\")\n",
    "print(f\"=\"*70)\n",
    "\n",
    "# Ejemplo 1: OR - documentos de chaperonas O genÃ©tica\n",
    "print(\"\\n1ï¸âƒ£ Filtro OR: tema = 'chaperonas' O tema = 'genetica'\")\n",
    "resultados_or = collection.query(\n",
    "    query_texts=[\"tÃ©cnicas de laboratorio\"],\n",
    "    n_results=4,\n",
    "    where={\n",
    "        \"$or\": [\n",
    "            {\"tema\": \"chaperonas\"},\n",
    "            {\"tema\": \"genetica\"}\n",
    "        ]\n",
    "    },\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(resultados_or['documents'][0]):\n",
    "    tema = resultados_or['metadatas'][0][i]['tema']\n",
    "    print(f\"   {i+1}. [{tema}] {doc[:60]}...\")\n",
    "\n",
    "# Ejemplo 2: AND + comparaciÃ³n numÃ©rica\n",
    "print(\"\\n2ï¸âƒ£ Filtro AND: tema = 'chaperonas' Y pagina < 10\")\n",
    "resultados_and = collection.query(\n",
    "    query_texts=[\"funciÃ³n proteica\"],\n",
    "    n_results=4,\n",
    "    where={\n",
    "        \"$and\": [\n",
    "            {\"tema\": \"chaperonas\"},\n",
    "            {\"pagina\": {\"$lt\": 10}}  # $lt = less than\n",
    "        ]\n",
    "    },\n",
    "    include=[\"documents\", \"metadatas\"]\n",
    ")\n",
    "\n",
    "for i, doc in enumerate(resultados_and['documents'][0]):\n",
    "    pagina = resultados_and['metadatas'][0][i]['pagina']\n",
    "    print(f\"   {i+1}. [pÃ¡g. {pagina}] {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 ImplementaciÃ³n con FAISS\n",
    "\n",
    "**FAISS** (Facebook AI Similarity Search) es una biblioteca de bajo nivel que ofrece mÃ¡ximo rendimiento. Aunque requiere mÃ¡s cÃ³digo, es ideal cuando se necesita escala o control fino sobre el Ã­ndice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Cargar modelo de embeddings (reusar el anterior si ya estÃ¡ cargado)\n",
    "if 'embedding_model' not in dir():\n",
    "    embedding_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "# Generar embeddings para los documentos\n",
    "embeddings_docs = embedding_model.encode(documentos)\n",
    "embeddings_docs = embeddings_docs.astype('float32')  # FAISS requiere float32\n",
    "\n",
    "# Normalizar para usar producto punto (equivalente a coseno)\n",
    "faiss.normalize_L2(embeddings_docs)\n",
    "\n",
    "# Crear Ã­ndice FAISS\n",
    "dimension = embeddings_docs.shape[1]\n",
    "index = faiss.IndexFlatIP(dimension)  # IP = Inner Product (coseno con normalizaciÃ³n)\n",
    "\n",
    "# AÃ±adir vectores al Ã­ndice\n",
    "index.add(embeddings_docs)\n",
    "\n",
    "print(f\"âœ… Ãndice FAISS creado\")\n",
    "print(f\"   DimensiÃ³n: {dimension}\")\n",
    "print(f\"   Vectores indexados: {index.ntotal}\")\n",
    "print(f\"   Tipo de Ã­ndice: Flat (bÃºsqueda exacta)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# BÃºsqueda con FAISS\n",
    "# ============================================================\n",
    "\n",
    "def buscar_faiss(consulta, index, modelo, documentos, k=3):\n",
    "    \"\"\"Realiza bÃºsqueda semÃ¡ntica usando FAISS.\"\"\"\n",
    "    # Generar y normalizar embedding de consulta\n",
    "    query_embedding = modelo.encode([consulta]).astype('float32')\n",
    "    faiss.normalize_L2(query_embedding)\n",
    "    \n",
    "    # Buscar los k vectores mÃ¡s cercanos\n",
    "    scores, indices = index.search(query_embedding, k)\n",
    "    \n",
    "    # Formatear resultados\n",
    "    resultados = []\n",
    "    for i, (idx, score) in enumerate(zip(indices[0], scores[0])):\n",
    "        resultados.append({\n",
    "            'documento': documentos[idx],\n",
    "            'score': score,\n",
    "            'indice': idx\n",
    "        })\n",
    "    \n",
    "    return resultados\n",
    "\n",
    "\n",
    "# Realizar bÃºsqueda\n",
    "consulta = \"proteÃ­nas relacionadas con enfermedades cerebrales\"\n",
    "\n",
    "print(f\"ğŸ” BÃšSQUEDA CON FAISS\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Consulta: \\\"{consulta}\\\"\\n\")\n",
    "\n",
    "resultados_faiss = buscar_faiss(consulta, index, embedding_model, documentos, k=3)\n",
    "\n",
    "for i, r in enumerate(resultados_faiss, 1):\n",
    "    print(f\"{i}. [Score: {r['score']:.4f}]\")\n",
    "    print(f\"   {r['documento'][:80]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Mejores PrÃ¡cticas para Vector Stores\n",
    "\n",
    "### DiseÃ±o de Colecciones\n",
    "\n",
    "1. **Separar por dominio o tipo de documento**\n",
    "   - Una colecciÃ³n para papers cientÃ­ficos\n",
    "   - Otra para documentaciÃ³n tÃ©cnica\n",
    "   - Esto optimiza las bÃºsquedas y permite configuraciones especÃ­ficas\n",
    "\n",
    "2. **Metadatos estratÃ©gicos**\n",
    "   - Incluir informaciÃ³n que permita filtrar (fecha, autor, categorÃ­a)\n",
    "   - Guardar referencia a la fuente original (archivo, URL, pÃ¡gina)\n",
    "   - Considerar metadatos para control de acceso si es necesario\n",
    "\n",
    "### OptimizaciÃ³n de Rendimiento\n",
    "\n",
    "| Factor | RecomendaciÃ³n |\n",
    "|--------|---------------|\n",
    "| < 10K docs | Ãndice Flat (exacto) |\n",
    "| 10K - 1M docs | HNSW con parÃ¡metros default |\n",
    "| > 1M docs | IVF + tuning de parÃ¡metros |\n",
    "| Filtros frecuentes | Crear Ã­ndices en campos de metadatos |\n",
    "\n",
    "### Mantenimiento\n",
    "\n",
    "1. **Actualizaciones incrementales**: AÃ±adir nuevos documentos sin reindexar todo\n",
    "2. **Versionado**: Mantener versiones del Ã­ndice para rollback\n",
    "3. **Backups**: Respaldar tanto los vectores como los metadatos\n",
    "4. **MonitorizaciÃ³n**: Tracking de latencias y calidad de resultados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"5-pipeline\"></a>\n",
    "# 5. Pipeline RAG Completo\n",
    "\n",
    "Ahora que entendemos cada componente (embeddings, chunking, vector stores), vamos a integrar todo en un **pipeline RAG funcional**.\n",
    "\n",
    "## 5.1 Arquitectura del Pipeline\n",
    "\n",
    "Un pipeline RAG completo tiene las siguientes etapas:\n",
    "\n",
    "```\n",
    "                    FASE DE INDEXACIÃ“N\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                          â”‚\n",
    "â”‚  [Documentos] â†’ [Chunking] â†’ [Embedding] â†’ [VectorStore] â”‚\n",
    "â”‚                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "                    FASE DE CONSULTA\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                                                          â”‚\n",
    "â”‚  [Query] â†’ [Embedding] â†’ [Retrieval] â†’ [Contexto]       â”‚\n",
    "â”‚                              â”‚                           â”‚\n",
    "â”‚                              â†“                           â”‚\n",
    "â”‚  [Respuesta] â† [LLM] â† [Prompt con Contexto]            â”‚\n",
    "â”‚                                                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## 5.2 ImplementaciÃ³n Paso a Paso\n",
    "\n",
    "Vamos a construir el pipeline de forma modular, componente por componente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# CLASE RAGPipeline: ImplementaciÃ³n completa\n",
    "# ============================================================\n",
    "\n",
    "from typing import List, Dict, Optional\n",
    "from dataclasses import dataclass\n",
    "import chromadb\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "@dataclass\n",
    "class Documento:\n",
    "    \"\"\"Representa un documento con su contenido y metadatos.\"\"\"\n",
    "    contenido: str\n",
    "    metadata: Dict = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ResultadoBusqueda:\n",
    "    \"\"\"Representa un resultado de bÃºsqueda.\"\"\"\n",
    "    contenido: str\n",
    "    score: float\n",
    "    metadata: Dict\n",
    "\n",
    "\n",
    "class RAGPipeline:\n",
    "    \"\"\"\n",
    "    Pipeline RAG completo con indexaciÃ³n, bÃºsqueda y generaciÃ³n.\n",
    "    \n",
    "    Esta implementaciÃ³n estÃ¡ diseÃ±ada para ser didÃ¡ctica y mostrar\n",
    "    claramente cada paso del proceso RAG.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        embedding_model: str = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "        collection_name: str = \"rag_collection\",\n",
    "        chunk_size: int = 500,\n",
    "        chunk_overlap: int = 100\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Inicializa el pipeline RAG.\n",
    "        \n",
    "        Args:\n",
    "            embedding_model: Nombre del modelo de embeddings\n",
    "            collection_name: Nombre de la colecciÃ³n en ChromaDB\n",
    "            chunk_size: TamaÃ±o mÃ¡ximo de cada chunk en caracteres\n",
    "            chunk_overlap: Solapamiento entre chunks\n",
    "        \"\"\"\n",
    "        print(\"ğŸ”§ Inicializando RAG Pipeline...\")\n",
    "        \n",
    "        # Modelo de embeddings\n",
    "        print(f\"   ğŸ“ Cargando modelo de embeddings: {embedding_model}\")\n",
    "        self.embedding_model = SentenceTransformer(embedding_model)\n",
    "        \n",
    "        # Splitter para chunking\n",
    "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size,\n",
    "            chunk_overlap=chunk_overlap,\n",
    "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \"]\n",
    "        )\n",
    "        \n",
    "        # Vector store (ChromaDB)\n",
    "        print(f\"   ğŸ“¦ Configurando ChromaDB: {collection_name}\")\n",
    "        self.client = chromadb.Client()\n",
    "        \n",
    "        # FunciÃ³n de embedding para ChromaDB\n",
    "        self.embedding_fn = chromadb.utils.embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "            model_name=embedding_model\n",
    "        )\n",
    "        \n",
    "        self.collection = self.client.get_or_create_collection(\n",
    "            name=collection_name,\n",
    "            embedding_function=self.embedding_fn,\n",
    "            metadata={\"hnsw:space\": \"cosine\"}\n",
    "        )\n",
    "        \n",
    "        # Contador para IDs\n",
    "        self._doc_counter = 0\n",
    "        \n",
    "        print(\"âœ… Pipeline inicializado correctamente\\n\")\n",
    "    \n",
    "    def indexar_documentos(self, documentos: List[Documento]) -> int:\n",
    "        \"\"\"\n",
    "        Indexa una lista de documentos en el vector store.\n",
    "        \n",
    "        Proceso:\n",
    "        1. Divide cada documento en chunks\n",
    "        2. Genera embeddings (automÃ¡tico por ChromaDB)\n",
    "        3. Almacena en el vector store\n",
    "        \n",
    "        Returns:\n",
    "            NÃºmero de chunks indexados\n",
    "        \"\"\"\n",
    "        print(f\"ğŸ“¥ Indexando {len(documentos)} documentos...\")\n",
    "        \n",
    "        todos_chunks = []\n",
    "        todos_metadatos = []\n",
    "        todos_ids = []\n",
    "        \n",
    "        for doc in documentos:\n",
    "            # Dividir en chunks\n",
    "            chunks = self.text_splitter.split_text(doc.contenido)\n",
    "            \n",
    "            for i, chunk in enumerate(chunks):\n",
    "                # Preparar metadatos del chunk\n",
    "                chunk_metadata = doc.metadata.copy()\n",
    "                chunk_metadata['chunk_index'] = i\n",
    "                chunk_metadata['total_chunks'] = len(chunks)\n",
    "                \n",
    "                todos_chunks.append(chunk)\n",
    "                todos_metadatos.append(chunk_metadata)\n",
    "                todos_ids.append(f\"doc_{self._doc_counter:05d}\")\n",
    "                self._doc_counter += 1\n",
    "        \n",
    "        # AÃ±adir a ChromaDB\n",
    "        if todos_chunks:\n",
    "            self.collection.add(\n",
    "                documents=todos_chunks,\n",
    "                metadatas=todos_metadatos,\n",
    "                ids=todos_ids\n",
    "            )\n",
    "        \n",
    "        print(f\"âœ… {len(todos_chunks)} chunks indexados\")\n",
    "        return len(todos_chunks)\n",
    "    \n",
    "    def buscar(self, consulta: str, k: int = 3, filtros: Dict = None) -> List[ResultadoBusqueda]:\n",
    "        \"\"\"\n",
    "        Busca los documentos mÃ¡s relevantes para una consulta.\n",
    "        \n",
    "        Args:\n",
    "            consulta: Pregunta o texto de bÃºsqueda\n",
    "            k: NÃºmero de resultados a devolver\n",
    "            filtros: Diccionario de filtros de metadatos (opcional)\n",
    "        \n",
    "        Returns:\n",
    "            Lista de ResultadoBusqueda ordenados por relevancia\n",
    "        \"\"\"\n",
    "        resultados = self.collection.query(\n",
    "            query_texts=[consulta],\n",
    "            n_results=k,\n",
    "            where=filtros,\n",
    "            include=[\"documents\", \"metadatas\", \"distances\"]\n",
    "        )\n",
    "        \n",
    "        # Convertir a objetos ResultadoBusqueda\n",
    "        busquedas = []\n",
    "        for i in range(len(resultados['documents'][0])):\n",
    "            busquedas.append(ResultadoBusqueda(\n",
    "                contenido=resultados['documents'][0][i],\n",
    "                score=1 - resultados['distances'][0][i],  # Convertir distancia a similitud\n",
    "                metadata=resultados['metadatas'][0][i]\n",
    "            ))\n",
    "        \n",
    "        return busquedas\n",
    "    \n",
    "    def construir_contexto(self, resultados: List[ResultadoBusqueda]) -> str:\n",
    "        \"\"\"\n",
    "        Construye el contexto para el LLM a partir de los resultados.\n",
    "        \n",
    "        Formatea los documentos recuperados de forma estructurada\n",
    "        para que el LLM pueda utilizarlos efectivamente.\n",
    "        \"\"\"\n",
    "        partes_contexto = []\n",
    "        \n",
    "        for i, res in enumerate(resultados, 1):\n",
    "            fuente = res.metadata.get('fuente', 'Desconocida')\n",
    "            pagina = res.metadata.get('pagina', 'N/A')\n",
    "            \n",
    "            parte = f\"[Documento {i} - Fuente: {fuente}, PÃ¡g. {pagina}]\\n{res.contenido}\"\n",
    "            partes_contexto.append(parte)\n",
    "        \n",
    "        return \"\\n\\n\".join(partes_contexto)\n",
    "    \n",
    "    def generar_prompt(self, consulta: str, contexto: str) -> str:\n",
    "        \"\"\"\n",
    "        Genera el prompt completo para el LLM.\n",
    "        \n",
    "        Este prompt estÃ¡ diseÃ±ado para:\n",
    "        - Instruir al LLM a usar solo el contexto proporcionado\n",
    "        - Solicitar citaciones de las fuentes\n",
    "        - Manejar casos donde no hay informaciÃ³n suficiente\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Eres un asistente de investigaciÃ³n cientÃ­fica. Tu tarea es responder \n",
    "preguntas basÃ¡ndote ÃšNICAMENTE en el contexto proporcionado.\n",
    "\n",
    "REGLAS IMPORTANTES:\n",
    "1. Usa SOLO la informaciÃ³n del contexto para responder\n",
    "2. Si la informaciÃ³n no estÃ¡ en el contexto, di: \"No tengo informaciÃ³n sobre esto en los documentos disponibles\"\n",
    "3. Cita las fuentes usando el formato [Documento X]\n",
    "4. SÃ© preciso y conciso en tu respuesta\n",
    "5. Si hay informaciÃ³n contradictoria, menciona ambas perspectivas\n",
    "\n",
    "CONTEXTO:\n",
    "---\n",
    "{contexto}\n",
    "---\n",
    "\n",
    "PREGUNTA: {consulta}\n",
    "\n",
    "RESPUESTA:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def consultar(self, pregunta: str, k: int = 3, filtros: Dict = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Ejecuta el pipeline RAG completo.\n",
    "        \n",
    "        Esta funciÃ³n combina todos los pasos:\n",
    "        1. BÃºsqueda de documentos relevantes\n",
    "        2. ConstrucciÃ³n del contexto\n",
    "        3. GeneraciÃ³n del prompt\n",
    "        \n",
    "        Returns:\n",
    "            Diccionario con prompt, contexto y documentos fuente\n",
    "        \"\"\"\n",
    "        # Paso 1: Buscar documentos relevantes\n",
    "        resultados = self.buscar(pregunta, k=k, filtros=filtros)\n",
    "        \n",
    "        # Paso 2: Construir contexto\n",
    "        contexto = self.construir_contexto(resultados)\n",
    "        \n",
    "        # Paso 3: Generar prompt\n",
    "        prompt = self.generar_prompt(pregunta, contexto)\n",
    "        \n",
    "        return {\n",
    "            'prompt': prompt,\n",
    "            'contexto': contexto,\n",
    "            'documentos_fuente': resultados,\n",
    "            'num_documentos': len(resultados)\n",
    "        }\n",
    "\n",
    "\n",
    "print(\"âœ… Clase RAGPipeline definida correctamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Crear y usar el pipeline\n",
    "# ============================================================\n",
    "\n",
    "# Inicializar el pipeline\n",
    "rag = RAGPipeline(\n",
    "    embedding_model=\"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\",\n",
    "    collection_name=\"demo_rag\",\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Preparar e indexar documentos\n",
    "# ============================================================\n",
    "\n",
    "# Crear documentos de ejemplo\n",
    "docs_ejemplo = [\n",
    "    Documento(\n",
    "        contenido=\"\"\"Las proteÃ­nas chaperonas son molÃ©culas esenciales para la vida celular.\n",
    "        Funcionan como \"guardianes\" del plegamiento proteico, asistiendo a otras proteÃ­nas\n",
    "        para que adopten su estructura tridimensional correcta. Sin las chaperonas, muchas\n",
    "        proteÃ­nas se plegarÃ­an incorrectamente y formarÃ­an agregados tÃ³xicos.\n",
    "        \n",
    "        El mecanismo de acciÃ³n implica ciclos de uniÃ³n y liberaciÃ³n dependientes de ATP.\n",
    "        Las chaperonas reconocen regiones hidrofÃ³bicas expuestas, que normalmente estarÃ­an\n",
    "        ocultas en el interior de una proteÃ­na correctamente plegada.\"\"\",\n",
    "        metadata={\"fuente\": \"manual_bioquimica.pdf\", \"pagina\": 245, \"tema\": \"chaperonas\"}\n",
    "    ),\n",
    "    Documento(\n",
    "        contenido=\"\"\"La familia Hsp70 es una de las mÃ¡s importantes y conservadas evolutivamente.\n",
    "        Estas chaperonas reconocen segmentos hidrofÃ³bicos de 7-8 aminoÃ¡cidos en proteÃ­nas\n",
    "        desplegadas. Para funcionar Ã³ptimamente, Hsp70 requiere co-chaperonas como Hsp40\n",
    "        (tambiÃ©n conocida como DnaJ), que estimulan su actividad ATPasa.\n",
    "        \n",
    "        El ciclo funcional de Hsp70 incluye:\n",
    "        1. UniÃ³n del sustrato en estado ATP\n",
    "        2. HidrÃ³lisis de ATP estimulada por Hsp40\n",
    "        3. LiberaciÃ³n del sustrato tras intercambio de nucleÃ³tido\"\"\",\n",
    "        metadata={\"fuente\": \"manual_bioquimica.pdf\", \"pagina\": 248, \"tema\": \"chaperonas\"}\n",
    "    ),\n",
    "    Documento(\n",
    "        contenido=\"\"\"El mal plegamiento de proteÃ­nas estÃ¡ implicado en numerosas enfermedades\n",
    "        neurodegenerativas. En la enfermedad de Alzheimer, los agregados de beta-amiloide\n",
    "        y tau hiperfosforilada forman placas y ovillos neurofibrilares respectivamente.\n",
    "        \n",
    "        En Parkinson, la proteÃ­na alfa-sinucleÃ­na se agrega formando los cuerpos de Lewy.\n",
    "        Las enfermedades por priones son causadas por PrPSc, una forma mal plegada de la\n",
    "        proteÃ­na priÃ³nica que puede \"contagiar\" su conformaciÃ³n a proteÃ­nas normales.\"\"\",\n",
    "        metadata={\"fuente\": \"neurologia_molecular.pdf\", \"pagina\": 89, \"tema\": \"patologia\"}\n",
    "    ),\n",
    "    Documento(\n",
    "        contenido=\"\"\"CRISPR-Cas9 ha revolucionado la ediciÃ³n genÃ©tica. Este sistema utiliza\n",
    "        un ARN guÃ­a para dirigir la nucleasa Cas9 a una secuencia especÃ­fica del genoma,\n",
    "        donde realiza un corte de doble hebra. La cÃ©lula repara este corte mediante\n",
    "        uniÃ³n de extremos no homÃ³logos (NHEJ) o recombinaciÃ³n homÃ³loga (HDR).\n",
    "        \n",
    "        Aplicaciones incluyen: correcciÃ³n de mutaciones genÃ©ticas, creaciÃ³n de modelos\n",
    "        de enfermedad, desarrollo de terapias gÃ©nicas y mejora de cultivos.\"\"\",\n",
    "        metadata={\"fuente\": \"genetica_moderna.pdf\", \"pagina\": 156, \"tema\": \"genetica\"}\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Indexar documentos\n",
    "num_chunks = rag.indexar_documentos(docs_ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Realizar consultas con el pipeline\n",
    "# ============================================================\n",
    "\n",
    "pregunta = \"Â¿CÃ³mo funcionan las proteÃ­nas chaperonas y quÃ© enfermedades estÃ¡n relacionadas con su mal funcionamiento?\"\n",
    "\n",
    "print(\"ğŸ” CONSULTA RAG\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Pregunta: {pregunta}\\n\")\n",
    "\n",
    "# Ejecutar pipeline\n",
    "resultado = rag.consultar(pregunta, k=3)\n",
    "\n",
    "# Mostrar documentos recuperados\n",
    "print(\"ğŸ“„ DOCUMENTOS RECUPERADOS:\")\n",
    "print(\"-\"*70)\n",
    "for i, doc in enumerate(resultado['documentos_fuente'], 1):\n",
    "    print(f\"\\n{i}. [Similitud: {doc.score:.4f}]\")\n",
    "    print(f\"   Fuente: {doc.metadata.get('fuente')}, pÃ¡g. {doc.metadata.get('pagina')}\")\n",
    "    print(f\"   {doc.contenido[:100]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“ PROMPT GENERADO PARA EL LLM:\")\n",
    "print(\"-\"*70)\n",
    "print(resultado['prompt'][:1500] + \"...\\n\")\n",
    "\n",
    "print(\"ğŸ’¡ Este prompt se enviarÃ­a al LLM (Llama, Mistral, etc.) para generar la respuesta final.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"6-avanzadas\"></a>\n",
    "# 6. TÃ©cnicas Avanzadas de Retrieval\n",
    "\n",
    "El RAG bÃ¡sico (\"Naive RAG\") funciona bien en muchos casos, pero existen tÃ©cnicas avanzadas que pueden mejorar significativamente la calidad de las respuestas.\n",
    "\n",
    "## 6.1 Limitaciones del RAG BÃ¡sico\n",
    "\n",
    "| LimitaciÃ³n | DescripciÃ³n | Impacto |\n",
    "|------------|-------------|----------|\n",
    "| **BÃºsqueda solo semÃ¡ntica** | No captura coincidencias de tÃ©rminos exactos | Falla con tÃ©rminos tÃ©cnicos especÃ­ficos |\n",
    "| **Sin reordenamiento** | Primer resultado puede no ser el mejor | Contexto subÃ³ptimo para el LLM |\n",
    "| **Query Ãºnica** | La consulta del usuario puede ser ambigua | Baja cobertura de documentos relevantes |\n",
    "| **Sin verificaciÃ³n** | El LLM puede ignorar el contexto | Posibles alucinaciones |\n",
    "\n",
    "## 6.2 Hybrid Search: Combinando SemÃ¡ntica y Keywords\n",
    "\n",
    "### El Problema\n",
    "\n",
    "La bÃºsqueda puramente semÃ¡ntica puede fallar con:\n",
    "- **TÃ©rminos tÃ©cnicos**: \"Hsp70\" vs \"proteÃ­na de choque tÃ©rmico 70\"\n",
    "- **AcrÃ³nimos**: \"PCR\", \"ADN\", \"ARNm\"\n",
    "- **Nombres propios**: \"CRISPR-Cas9\", \"Alzheimer\"\n",
    "\n",
    "### La SoluciÃ³n: BÃºsqueda HÃ­brida\n",
    "\n",
    "Combina dos tipos de bÃºsqueda:\n",
    "\n",
    "1. **BÃºsqueda Vectorial (SemÃ¡ntica)**: Captura el significado\n",
    "2. **BÃºsqueda LÃ©xica (BM25)**: Captura coincidencias de tÃ©rminos exactos\n",
    "\n",
    "```\n",
    "Score_final = Î± Ã— Score_vectorial + (1-Î±) Ã— Score_BM25\n",
    "\n",
    "donde Î± âˆˆ [0, 1] controla el balance:\n",
    "  Î± = 1.0 â†’ Solo vectorial\n",
    "  Î± = 0.0 â†’ Solo BM25\n",
    "  Î± = 0.5 â†’ Balance igual (recomendado como punto de partida)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ImplementaciÃ³n de BÃºsqueda HÃ­brida\n",
    "# ============================================================\n",
    "\n",
    "from rank_bm25 import BM25Okapi\n",
    "import re\n",
    "import numpy as np\n",
    "from typing import List, Tuple\n",
    "\n",
    "class BusquedaHibrida:\n",
    "    \"\"\"\n",
    "    Implementa bÃºsqueda hÃ­brida combinando vectorial + BM25.\n",
    "    \n",
    "    BM25 (Best Matching 25) es un algoritmo de ranking que considera:\n",
    "    - Frecuencia del tÃ©rmino en el documento (TF)\n",
    "    - Frecuencia inversa del documento (IDF)\n",
    "    - Longitud del documento\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, embedding_model):\n",
    "        self.embedding_model = embedding_model\n",
    "        self.documentos = []\n",
    "        self.embeddings = None\n",
    "        self.bm25 = None\n",
    "        self.tokenized_docs = []\n",
    "    \n",
    "    def _tokenizar(self, texto: str) -> List[str]:\n",
    "        \"\"\"TokenizaciÃ³n simple para BM25.\"\"\"\n",
    "        # Convertir a minÃºsculas y extraer palabras\n",
    "        tokens = re.findall(r'\\w+', texto.lower())\n",
    "        return tokens\n",
    "    \n",
    "    def indexar(self, documentos: List[str]):\n",
    "        \"\"\"Indexa documentos para bÃºsqueda hÃ­brida.\"\"\"\n",
    "        self.documentos = documentos\n",
    "        \n",
    "        # Ãndice vectorial\n",
    "        print(\"   Generando embeddings...\")\n",
    "        self.embeddings = self.embedding_model.encode(documentos)\n",
    "        \n",
    "        # Ãndice BM25\n",
    "        print(\"   Construyendo Ã­ndice BM25...\")\n",
    "        self.tokenized_docs = [self._tokenizar(doc) for doc in documentos]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_docs)\n",
    "        \n",
    "        print(f\"âœ… {len(documentos)} documentos indexados para bÃºsqueda hÃ­brida\")\n",
    "    \n",
    "    def buscar(self, query: str, k: int = 5, alpha: float = 0.5) -> List[Tuple[str, float, dict]]:\n",
    "        \"\"\"\n",
    "        Realiza bÃºsqueda hÃ­brida.\n",
    "        \n",
    "        Args:\n",
    "            query: Consulta del usuario\n",
    "            k: NÃºmero de resultados\n",
    "            alpha: Peso de bÃºsqueda vectorial (0-1)\n",
    "                   1.0 = solo vectorial, 0.0 = solo BM25\n",
    "        \n",
    "        Returns:\n",
    "            Lista de (documento, score_hibrido, detalles)\n",
    "        \"\"\"\n",
    "        # BÃºsqueda vectorial\n",
    "        query_embedding = self.embedding_model.encode([query])\n",
    "        vector_scores = cosine_similarity(query_embedding, self.embeddings)[0]\n",
    "        \n",
    "        # BÃºsqueda BM25\n",
    "        tokenized_query = self._tokenizar(query)\n",
    "        bm25_scores = self.bm25.get_scores(tokenized_query)\n",
    "        \n",
    "        # Normalizar scores a rango [0, 1]\n",
    "        vector_norm = self._normalizar(vector_scores)\n",
    "        bm25_norm = self._normalizar(bm25_scores)\n",
    "        \n",
    "        # Combinar scores\n",
    "        hybrid_scores = alpha * vector_norm + (1 - alpha) * bm25_norm\n",
    "        \n",
    "        # Obtener top-k\n",
    "        top_indices = np.argsort(hybrid_scores)[::-1][:k]\n",
    "        \n",
    "        resultados = []\n",
    "        for idx in top_indices:\n",
    "            resultados.append((\n",
    "                self.documentos[idx],\n",
    "                hybrid_scores[idx],\n",
    "                {\n",
    "                    'score_vectorial': vector_scores[idx],\n",
    "                    'score_bm25': bm25_scores[idx],\n",
    "                    'score_vectorial_norm': vector_norm[idx],\n",
    "                    'score_bm25_norm': bm25_norm[idx]\n",
    "                }\n",
    "            ))\n",
    "        \n",
    "        return resultados\n",
    "    \n",
    "    def _normalizar(self, scores):\n",
    "        \"\"\"Normaliza scores al rango [0, 1].\"\"\"\n",
    "        min_s, max_s = scores.min(), scores.max()\n",
    "        if max_s - min_s < 1e-6:\n",
    "            return np.zeros_like(scores)\n",
    "        return (scores - min_s) / (max_s - min_s)\n",
    "\n",
    "\n",
    "# Crear instancia\n",
    "print(\"ğŸ”§ Inicializando bÃºsqueda hÃ­brida...\")\n",
    "hibrida = BusquedaHibrida(embedding_model)\n",
    "\n",
    "# Indexar documentos de ejemplo\n",
    "docs_hibrida = [\n",
    "    \"Las proteÃ­nas chaperonas Hsp70 y Hsp90 son esenciales para el plegamiento proteico.\",\n",
    "    \"La tÃ©cnica CRISPR-Cas9 permite ediciÃ³n genÃ©tica precisa del ADN.\",\n",
    "    \"El mal plegamiento de proteÃ­nas causa enfermedades como Alzheimer.\",\n",
    "    \"Hsp70 reconoce secuencias hidrofÃ³bicas de 7-8 aminoÃ¡cidos.\",\n",
    "    \"La PCR amplifica secuencias especÃ­ficas de ADN usando Taq polimerasa.\",\n",
    "    \"Los agregados de alfa-sinucleÃ­na causan la enfermedad de Parkinson.\",\n",
    "]\n",
    "\n",
    "hibrida.indexar(docs_hibrida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ComparaciÃ³n: Vectorial vs BM25 vs HÃ­brido\n",
    "# ============================================================\n",
    "\n",
    "# Query con tÃ©rmino tÃ©cnico especÃ­fico\n",
    "query = \"Hsp70 aminoÃ¡cidos hidrofÃ³bicos\"\n",
    "\n",
    "print(f\"ğŸ” COMPARACIÃ“N DE MÃ‰TODOS DE BÃšSQUEDA\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "# Solo vectorial (alpha=1.0)\n",
    "print(\"1ï¸âƒ£ SOLO VECTORIAL (alpha=1.0):\")\n",
    "res_vec = hibrida.buscar(query, k=3, alpha=1.0)\n",
    "for i, (doc, score, _) in enumerate(res_vec, 1):\n",
    "    print(f\"   {i}. [{score:.3f}] {doc[:60]}...\")\n",
    "\n",
    "# Solo BM25 (alpha=0.0)\n",
    "print(\"\\n2ï¸âƒ£ SOLO BM25 (alpha=0.0):\")\n",
    "res_bm25 = hibrida.buscar(query, k=3, alpha=0.0)\n",
    "for i, (doc, score, _) in enumerate(res_bm25, 1):\n",
    "    print(f\"   {i}. [{score:.3f}] {doc[:60]}...\")\n",
    "\n",
    "# HÃ­brido (alpha=0.5)\n",
    "print(\"\\n3ï¸âƒ£ HÃBRIDO (alpha=0.5):\")\n",
    "res_hyb = hibrida.buscar(query, k=3, alpha=0.5)\n",
    "for i, (doc, score, detalles) in enumerate(res_hyb, 1):\n",
    "    print(f\"   {i}. [{score:.3f}] (vec:{detalles['score_vectorial_norm']:.2f}, bm25:{detalles['score_bm25_norm']:.2f})\")\n",
    "    print(f\"      {doc[:55]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¡ Observa cÃ³mo BM25 encuentra mejor el documento con 'Hsp70' y 'aminoÃ¡cidos',\")\n",
    "print(\"   mientras que vectorial captura el significado general.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Reranking: Mejorando la PrecisiÃ³n\n",
    "\n",
    "### El Concepto\n",
    "\n",
    "El **reranking** es un paso adicional que reordena los resultados iniciales usando un modelo mÃ¡s preciso pero mÃ¡s lento.\n",
    "\n",
    "```\n",
    "Query â†’ Retrieval (rÃ¡pido, top-20) â†’ Reranker (preciso, top-5) â†’ LLM\n",
    "```\n",
    "\n",
    "### Bi-Encoder vs Cross-Encoder\n",
    "\n",
    "| Aspecto | Bi-Encoder | Cross-Encoder |\n",
    "|---------|------------|---------------|\n",
    "| **CÃ³mo funciona** | Codifica query y doc por separado | Procesa query + doc juntos |\n",
    "| **Velocidad** | Muy rÃ¡pido (O(1) por comparaciÃ³n) | Lento (procesa cada par) |\n",
    "| **PrecisiÃ³n** | Buena | Excelente |\n",
    "| **Uso** | Retrieval inicial | Reranking |\n",
    "\n",
    "### Flujo TÃ­pico\n",
    "\n",
    "1. **Retrieval**: Bi-encoder recupera top-20 candidatos (rÃ¡pido)\n",
    "2. **Reranking**: Cross-encoder reordena â†’ top-5 (preciso)\n",
    "3. **GeneraciÃ³n**: LLM usa los 5 mejores documentos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ImplementaciÃ³n de Reranking con Cross-Encoder\n",
    "# ============================================================\n",
    "\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Cargar modelo de reranking\n",
    "print(\"ğŸ”§ Cargando modelo de reranking (Cross-Encoder)...\")\n",
    "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "print(\"âœ… Modelo de reranking cargado\\n\")\n",
    "\n",
    "\n",
    "def rerank(query: str, documentos: List[str], top_k: int = 3) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Reordena documentos usando Cross-Encoder.\n",
    "    \n",
    "    El Cross-Encoder evalÃºa directamente la relevancia de cada par\n",
    "    (query, documento), lo que es mÃ¡s preciso que comparar embeddings.\n",
    "    \"\"\"\n",
    "    # Crear pares (query, documento)\n",
    "    pares = [[query, doc] for doc in documentos]\n",
    "    \n",
    "    # Obtener scores del reranker\n",
    "    scores = reranker.predict(pares)\n",
    "    \n",
    "    # Ordenar por score descendente\n",
    "    doc_scores = list(zip(documentos, scores))\n",
    "    doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    return doc_scores[:top_k]\n",
    "\n",
    "\n",
    "# Ejemplo de reranking\n",
    "query = \"enfermedades causadas por proteÃ­nas que no se pliegan bien\"\n",
    "\n",
    "print(f\"ğŸ” DEMOSTRACIÃ“N DE RERANKING\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Query: \\\"{query}\\\"\\n\")\n",
    "\n",
    "# Orden original (simulando retrieval inicial)\n",
    "print(\"ğŸ“„ ORDEN ORIGINAL (del retrieval):\")\n",
    "for i, doc in enumerate(docs_hibrida, 1):\n",
    "    print(f\"   {i}. {doc[:60]}...\")\n",
    "\n",
    "# DespuÃ©s de reranking\n",
    "print(\"\\nğŸ“„ DESPUÃ‰S DE RERANKING:\")\n",
    "reranked = rerank(query, docs_hibrida, top_k=3)\n",
    "for i, (doc, score) in enumerate(reranked, 1):\n",
    "    print(f\"   {i}. [Score: {score:.4f}] {doc[:50]}...\")\n",
    "\n",
    "print(\"\\nğŸ’¡ El reranker identifica correctamente los documentos sobre\")\n",
    "print(\"   enfermedades y mal plegamiento como los mÃ¡s relevantes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Multi-Query RAG: Expandiendo la BÃºsqueda\n",
    "\n",
    "### El Problema\n",
    "\n",
    "Las consultas de los usuarios pueden ser:\n",
    "- **Ambiguas**: \"Â¿CÃ³mo funcionan las proteÃ­nas?\" (muy general)\n",
    "- **Incompletas**: No usan todos los tÃ©rminos relevantes\n",
    "- **Mal formuladas**: No coinciden con cÃ³mo estÃ¡ escrita la informaciÃ³n\n",
    "\n",
    "### La SoluciÃ³n: MÃºltiples Consultas\n",
    "\n",
    "Generar variaciones de la consulta original y fusionar los resultados:\n",
    "\n",
    "```\n",
    "Query original: \"Â¿QuÃ© hacen las chaperonas?\"\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â–¼               â–¼               â–¼\n",
    "   \"funciÃ³n de      \"cÃ³mo ayudan     \"proteÃ­nas Hsp70\n",
    "    chaperonas\"      las chaperonas\"   Hsp90 funciÃ³n\"\n",
    "        â”‚               â”‚               â”‚\n",
    "        â–¼               â–¼               â–¼\n",
    "    [Retrieval]    [Retrieval]    [Retrieval]\n",
    "        â”‚               â”‚               â”‚\n",
    "        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "                        â–¼\n",
    "                   [FusiÃ³n RRF]\n",
    "                        â”‚\n",
    "                        â–¼\n",
    "              Resultados combinados\n",
    "```\n",
    "\n",
    "### Reciprocal Rank Fusion (RRF)\n",
    "\n",
    "TÃ©cnica para combinar rankings de mÃºltiples fuentes:\n",
    "\n",
    "$$RRF(d) = \\sum_{q \\in Q} \\frac{1}{k + rank_q(d)}$$\n",
    "\n",
    "Donde:\n",
    "- $d$ = documento\n",
    "- $Q$ = conjunto de queries\n",
    "- $rank_q(d)$ = posiciÃ³n del documento $d$ en los resultados de query $q$\n",
    "- $k$ = constante (tÃ­picamente 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ImplementaciÃ³n de Multi-Query RAG\n",
    "# ============================================================\n",
    "\n",
    "def generar_queries_alternativas(query_original: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Genera variaciones de la consulta original.\n",
    "    \n",
    "    En producciÃ³n, usarÃ­as un LLM para generar estas variaciones.\n",
    "    AquÃ­ simulamos algunas transformaciones comunes.\n",
    "    \"\"\"\n",
    "    variaciones = [query_original]  # Siempre incluir la original\n",
    "    \n",
    "    # Ejemplo de variaciones manuales (en producciÃ³n, usar LLM)\n",
    "    if \"chaperona\" in query_original.lower():\n",
    "        variaciones.extend([\n",
    "            \"proteÃ­nas de choque tÃ©rmico HSP funciÃ³n\",\n",
    "            \"Hsp70 Hsp90 plegamiento proteico\",\n",
    "            \"cÃ³mo asisten las chaperonas moleculares\"\n",
    "        ])\n",
    "    elif \"enfermedad\" in query_original.lower():\n",
    "        variaciones.extend([\n",
    "            \"patologÃ­as proteÃ­nas mal plegadas\",\n",
    "            \"Alzheimer Parkinson agregados proteicos\"\n",
    "        ])\n",
    "    \n",
    "    return variaciones\n",
    "\n",
    "\n",
    "def reciprocal_rank_fusion(rankings: List[List[str]], k: int = 60) -> List[Tuple[str, float]]:\n",
    "    \"\"\"\n",
    "    Fusiona mÃºltiples rankings usando Reciprocal Rank Fusion.\n",
    "    \n",
    "    Args:\n",
    "        rankings: Lista de rankings (cada uno es una lista de documentos ordenados)\n",
    "        k: Constante de suavizado (default: 60)\n",
    "    \n",
    "    Returns:\n",
    "        Lista de (documento, score_rrf) ordenada por score\n",
    "    \"\"\"\n",
    "    rrf_scores = {}\n",
    "    \n",
    "    for ranking in rankings:\n",
    "        for rank, doc in enumerate(ranking, 1):\n",
    "            if doc not in rrf_scores:\n",
    "                rrf_scores[doc] = 0\n",
    "            rrf_scores[doc] += 1 / (k + rank)\n",
    "    \n",
    "    # Ordenar por score RRF\n",
    "    sorted_docs = sorted(rrf_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_docs\n",
    "\n",
    "\n",
    "# DemostraciÃ³n\n",
    "query_original = \"Â¿QuÃ© hacen las chaperonas moleculares?\"\n",
    "\n",
    "print(f\"ğŸ” MULTI-QUERY RAG\")\n",
    "print(f\"=\"*70)\n",
    "print(f\"Query original: \\\"{query_original}\\\"\\n\")\n",
    "\n",
    "# Generar queries alternativas\n",
    "queries = generar_queries_alternativas(query_original)\n",
    "print(\"ğŸ“ Queries generadas:\")\n",
    "for i, q in enumerate(queries, 1):\n",
    "    print(f\"   {i}. {q}\")\n",
    "\n",
    "# Realizar bÃºsquedas con cada query\n",
    "print(\"\\nğŸ” Resultados por query:\")\n",
    "rankings = []\n",
    "for q in queries:\n",
    "    resultados = hibrida.buscar(q, k=4, alpha=0.5)\n",
    "    ranking = [doc for doc, _, _ in resultados]\n",
    "    rankings.append(ranking)\n",
    "\n",
    "# Fusionar con RRF\n",
    "fusion = reciprocal_rank_fusion(rankings)\n",
    "\n",
    "print(\"\\nğŸ“„ RESULTADOS FUSIONADOS (RRF):\")\n",
    "for i, (doc, score) in enumerate(fusion[:4], 1):\n",
    "    print(f\"   {i}. [RRF: {score:.4f}] {doc[:55]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Resumen de TÃ©cnicas Avanzadas\n",
    "\n",
    "| TÃ©cnica | CuÃ¡ndo Usar | Mejora |\n",
    "|---------|-------------|--------|\n",
    "| **Hybrid Search** | TÃ©rminos tÃ©cnicos, acrÃ³nimos, nombres propios | Mejor recall para tÃ©rminos especÃ­ficos |\n",
    "| **Reranking** | Alta precisiÃ³n necesaria, tienes recursos computacionales | Mejor precisiÃ³n en top-k |\n",
    "| **Multi-Query** | Consultas ambiguas, baja cobertura | Mayor cobertura de documentos relevantes |\n",
    "| **Query Expansion** | Vocabulario variado en documentos | Mejor matching de tÃ©rminos |\n",
    "\n",
    "### Pipeline Avanzado Recomendado\n",
    "\n",
    "```\n",
    "Query â†’ [Multi-Query] â†’ [Hybrid Search] â†’ [Reranking] â†’ [LLM]\n",
    "         (expandir)      (recuperar)       (precisar)   (generar)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"7-evaluacion\"></a>\n",
    "# 7. EvaluaciÃ³n y MÃ©tricas de RAG\n",
    "\n",
    "## 7.1 Â¿Por quÃ© Evaluar?\n",
    "\n",
    "Sin evaluaciÃ³n sistemÃ¡tica, no puedes:\n",
    "- Saber si tu sistema RAG estÃ¡ funcionando bien\n",
    "- Comparar diferentes configuraciones\n",
    "- Detectar regresiones al hacer cambios\n",
    "- Justificar mejoras ante stakeholders\n",
    "\n",
    "## 7.2 MÃ©tricas de Retrieval\n",
    "\n",
    "EvalÃºan la calidad de los documentos recuperados:\n",
    "\n",
    "### Precision@k\n",
    "\n",
    "Â¿QuÃ© proporciÃ³n de los top-k documentos recuperados son relevantes?\n",
    "\n",
    "$$Precision@k = \\frac{|\\text{documentos relevantes en top-k}|}{k}$$\n",
    "\n",
    "**Ejemplo**: Si de los top-5 resultados, 3 son relevantes â†’ Precision@5 = 0.6\n",
    "\n",
    "### Recall@k\n",
    "\n",
    "Â¿QuÃ© proporciÃ³n de todos los documentos relevantes fueron recuperados?\n",
    "\n",
    "$$Recall@k = \\frac{|\\text{documentos relevantes en top-k}|}{|\\text{total documentos relevantes}|}$$\n",
    "\n",
    "**Ejemplo**: Si hay 10 docs relevantes y recuperamos 3 en top-5 â†’ Recall@5 = 0.3\n",
    "\n",
    "### MRR (Mean Reciprocal Rank)\n",
    "\n",
    "Â¿QuÃ© tan arriba aparece el primer documento relevante?\n",
    "\n",
    "$$MRR = \\frac{1}{|Q|} \\sum_{i=1}^{|Q|} \\frac{1}{rank_i}$$\n",
    "\n",
    "**Ejemplo**: Si el primer doc relevante estÃ¡ en posiciÃ³n 2 â†’ RR = 0.5\n",
    "\n",
    "### Hit Rate (Success Rate)\n",
    "\n",
    "Â¿Hay al menos un documento relevante en los top-k?\n",
    "\n",
    "$$HitRate@k = \\frac{|\\text{queries con al menos 1 doc relevante en top-k}|}{|\\text{total queries}|}$$\n",
    "\n",
    "## 7.3 MÃ©tricas de GeneraciÃ³n\n",
    "\n",
    "EvalÃºan la calidad de la respuesta del LLM:\n",
    "\n",
    "### Faithfulness (Fidelidad)\n",
    "\n",
    "Â¿La respuesta se basa en el contexto proporcionado?\n",
    "- Detecta alucinaciones\n",
    "- Verifica que no invente informaciÃ³n\n",
    "\n",
    "### Answer Relevancy\n",
    "\n",
    "Â¿La respuesta contesta la pregunta del usuario?\n",
    "- No basta con ser fiel al contexto\n",
    "- Debe ser pertinente a la consulta\n",
    "\n",
    "### Context Precision\n",
    "\n",
    "Â¿Los documentos del contexto eran realmente necesarios?\n",
    "- Penaliza incluir informaciÃ³n irrelevante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# ImplementaciÃ³n de MÃ©tricas de Retrieval\n",
    "# ============================================================\n",
    "\n",
    "def precision_at_k(retrieved: List[str], relevant: set, k: int) -> float:\n",
    "    \"\"\"\n",
    "    Calcula Precision@k.\n",
    "    \n",
    "    Args:\n",
    "        retrieved: Lista de documentos recuperados (ordenados por ranking)\n",
    "        relevant: Conjunto de documentos relevantes (ground truth)\n",
    "        k: NÃºmero de documentos a considerar\n",
    "    \"\"\"\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)\n",
    "    return relevant_in_k / k\n",
    "\n",
    "\n",
    "def recall_at_k(retrieved: List[str], relevant: set, k: int) -> float:\n",
    "    \"\"\"Calcula Recall@k.\"\"\"\n",
    "    if not relevant:\n",
    "        return 0.0\n",
    "    retrieved_k = retrieved[:k]\n",
    "    relevant_in_k = sum(1 for doc in retrieved_k if doc in relevant)\n",
    "    return relevant_in_k / len(relevant)\n",
    "\n",
    "\n",
    "def mrr(retrieved: List[str], relevant: set) -> float:\n",
    "    \"\"\"Calcula Mean Reciprocal Rank (para una query).\"\"\"\n",
    "    for rank, doc in enumerate(retrieved, 1):\n",
    "        if doc in relevant:\n",
    "            return 1 / rank\n",
    "    return 0.0\n",
    "\n",
    "\n",
    "def hit_rate_at_k(retrieved: List[str], relevant: set, k: int) -> float:\n",
    "    \"\"\"Calcula Hit Rate@k (1 si hay al menos un relevante, 0 si no).\"\"\"\n",
    "    retrieved_k = retrieved[:k]\n",
    "    return 1.0 if any(doc in relevant for doc in retrieved_k) else 0.0\n",
    "\n",
    "\n",
    "# Ejemplo de evaluaciÃ³n\n",
    "print(\"ğŸ“Š EVALUACIÃ“N DE RETRIEVAL\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Simulamos resultados de retrieval\n",
    "retrieved_docs = [\"doc_A\", \"doc_B\", \"doc_C\", \"doc_D\", \"doc_E\"]\n",
    "relevant_docs = {\"doc_A\", \"doc_C\", \"doc_F\", \"doc_G\"}  # Ground truth\n",
    "\n",
    "print(f\"\\nDocumentos recuperados: {retrieved_docs}\")\n",
    "print(f\"Documentos relevantes (ground truth): {relevant_docs}\")\n",
    "print()\n",
    "\n",
    "for k in [1, 3, 5]:\n",
    "    p = precision_at_k(retrieved_docs, relevant_docs, k)\n",
    "    r = recall_at_k(retrieved_docs, relevant_docs, k)\n",
    "    hr = hit_rate_at_k(retrieved_docs, relevant_docs, k)\n",
    "    print(f\"k={k}: Precision={p:.2f}, Recall={r:.2f}, Hit Rate={hr:.2f}\")\n",
    "\n",
    "print(f\"\\nMRR: {mrr(retrieved_docs, relevant_docs):.2f}\")\n",
    "print(\"   (El primer doc relevante 'doc_A' estÃ¡ en posiciÃ³n 1 â†’ MRR = 1.0)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"8-prompting\"></a>\n",
    "# 8. Prompt Engineering para RAG\n",
    "\n",
    "## 8.1 La Importancia del Prompt\n",
    "\n",
    "El prompt es la interfaz entre el contexto recuperado y la respuesta generada. Un buen prompt debe:\n",
    "\n",
    "1. **Instruir claramente** al modelo sobre su rol y tarea\n",
    "2. **Estructurar el contexto** de forma que sea fÃ¡cil de procesar\n",
    "3. **Guiar el formato** de la respuesta esperada\n",
    "4. **Manejar edge cases** (informaciÃ³n insuficiente, contradicciones)\n",
    "\n",
    "## 8.2 Estructura de un Prompt RAG Efectivo\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ SYSTEM PROMPT                                               â”‚\n",
    "â”‚ - Rol del asistente                                        â”‚\n",
    "â”‚ - Instrucciones de comportamiento                          â”‚\n",
    "â”‚ - Reglas para usar el contexto                             â”‚\n",
    "â”‚ - Formato de respuesta esperado                            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ CONTEXTO                                                    â”‚\n",
    "â”‚ - Documentos recuperados                                   â”‚\n",
    "â”‚ - Metadatos (fuente, pÃ¡gina, fecha)                        â”‚\n",
    "â”‚ - Delimitadores claros entre documentos                    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ PREGUNTA                                                    â”‚\n",
    "â”‚ - La consulta del usuario                                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ INSTRUCCIONES DE RESPUESTA                                  â”‚\n",
    "â”‚ - CÃ³mo estructurar la respuesta                            â”‚\n",
    "â”‚ - Requisitos de citaciÃ³n                                   â”‚\n",
    "â”‚ - QuÃ© hacer si no hay informaciÃ³n                          â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## 8.3 Plantillas de Prompt por Caso de Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Plantillas de Prompts para RAG\n",
    "# ============================================================\n",
    "\n",
    "PROMPTS_RAG = {\n",
    "    # Prompt para investigaciÃ³n cientÃ­fica\n",
    "    \"cientifico\": '''Eres un asistente de investigaciÃ³n cientÃ­fica especializado en {dominio}.\n",
    "\n",
    "INSTRUCCIONES:\n",
    "1. Responde ÃšNICAMENTE basÃ¡ndote en el contexto proporcionado\n",
    "2. Si la informaciÃ³n no estÃ¡ en el contexto, di claramente: \"No tengo informaciÃ³n sobre esto en los documentos disponibles\"\n",
    "3. Cita siempre las fuentes usando el formato [Documento X, pÃ¡g. Y]\n",
    "4. Usa terminologÃ­a cientÃ­fica precisa\n",
    "5. Si hay informaciÃ³n contradictoria, menciona ambas perspectivas\n",
    "6. Distingue entre hechos establecidos y hipÃ³tesis\n",
    "\n",
    "CONTEXTO:\n",
    "---\n",
    "{contexto}\n",
    "---\n",
    "\n",
    "PREGUNTA: {pregunta}\n",
    "\n",
    "RESPUESTA:''',\n",
    "\n",
    "    # Prompt para documentaciÃ³n tÃ©cnica\n",
    "    \"tecnico\": '''Eres un asistente tÃ©cnico que ayuda con documentaciÃ³n de sistemas y procedimientos.\n",
    "\n",
    "FORMATO DE RESPUESTA:\n",
    "1. Resumen breve (1-2 oraciones)\n",
    "2. Pasos detallados si es un procedimiento\n",
    "3. Advertencias o notas importantes\n",
    "4. Referencias a documentos especÃ­ficos\n",
    "\n",
    "REGLAS:\n",
    "- Usa solo la informaciÃ³n del contexto\n",
    "- SÃ© preciso con comandos, configuraciones y parÃ¡metros\n",
    "- Indica la versiÃ³n del sistema si estÃ¡ disponible\n",
    "\n",
    "DOCUMENTACIÃ“N DISPONIBLE:\n",
    "---\n",
    "{contexto}\n",
    "---\n",
    "\n",
    "CONSULTA: {pregunta}\n",
    "\n",
    "RESPUESTA:''',\n",
    "\n",
    "    # Prompt con output estructurado (JSON)\n",
    "    \"estructurado\": '''Analiza el contexto y responde la pregunta en formato JSON.\n",
    "\n",
    "CONTEXTO:\n",
    "---\n",
    "{contexto}\n",
    "---\n",
    "\n",
    "PREGUNTA: {pregunta}\n",
    "\n",
    "Responde con el siguiente formato JSON:\n",
    "{{\n",
    "    \"respuesta\": \"Tu respuesta basada en el contexto\",\n",
    "    \"confianza\": \"alta|media|baja\",\n",
    "    \"fuentes_usadas\": [\"lista de fuentes citadas\"],\n",
    "    \"informacion_faltante\": \"quÃ© informaciÃ³n adicional serÃ­a Ãºtil (o null si la respuesta es completa)\"\n",
    "}}\n",
    "\n",
    "JSON:''',\n",
    "\n",
    "    # Prompt simple para casos bÃ¡sicos\n",
    "    \"simple\": '''Contexto:\n",
    "{contexto}\n",
    "\n",
    "Pregunta: {pregunta}\n",
    "\n",
    "Responde basÃ¡ndote solo en el contexto. Si no hay informaciÃ³n suficiente, dilo claramente.\n",
    "\n",
    "Respuesta:'''\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ PLANTILLAS DE PROMPTS DISPONIBLES:\")\n",
    "print(\"=\"*50)\n",
    "for nombre in PROMPTS_RAG.keys():\n",
    "    print(f\"   âœ… {nombre}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.4 TÃ©cnicas Avanzadas de Prompting\n",
    "\n",
    "### Chain of Thought (CoT)\n",
    "\n",
    "Instruir al modelo a razonar paso a paso:\n",
    "\n",
    "```\n",
    "\"Antes de responder, analiza el contexto paso a paso:\n",
    "1. Identifica los conceptos clave en la pregunta\n",
    "2. Localiza informaciÃ³n relevante en cada documento\n",
    "3. Sintetiza la informaciÃ³n encontrada\n",
    "4. Formula una respuesta coherente\"\n",
    "```\n",
    "\n",
    "### Few-Shot Learning\n",
    "\n",
    "Proporcionar ejemplos de respuestas deseadas:\n",
    "\n",
    "```\n",
    "\"Ejemplos de respuestas correctas:\n",
    "\n",
    "Pregunta: Â¿QuÃ© es X?\n",
    "Respuesta: X es... [Documento 1, pÃ¡g. 5]\n",
    "\n",
    "Pregunta: Â¿CÃ³mo funciona Y?\n",
    "Respuesta: Y funciona mediante... [Documento 2, pÃ¡g. 12]\"\n",
    "```\n",
    "\n",
    "### Self-Consistency\n",
    "\n",
    "Generar mÃºltiples respuestas y seleccionar la mÃ¡s consistente.\n",
    "\n",
    "### Guardrails\n",
    "\n",
    "Validar que la respuesta cumpla criterios especÃ­ficos:\n",
    "- Â¿Cita fuentes?\n",
    "- Â¿Responde la pregunta?\n",
    "- Â¿Es coherente con el contexto?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"9-opensource\"></a>\n",
    "# 9. Soluciones Open Source para RAG\n",
    "\n",
    "## 9.1 Frameworks de Desarrollo\n",
    "\n",
    "### LangChain\n",
    "\n",
    "El framework mÃ¡s popular para construir aplicaciones con LLMs.\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|----------|\n",
    "| **Licencia** | MIT |\n",
    "| **Lenguajes** | Python, JavaScript |\n",
    "| **Integraciones** | 100+ (LLMs, vectorstores, tools) |\n",
    "| **DocumentaciÃ³n** | Excelente |\n",
    "| **Comunidad** | Muy activa |\n",
    "| **Ideal para** | Prototipos rÃ¡pidos, producciÃ³n |\n",
    "\n",
    "### LlamaIndex\n",
    "\n",
    "Especializado en indexaciÃ³n y recuperaciÃ³n de datos.\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|----------|\n",
    "| **Licencia** | MIT |\n",
    "| **Fortaleza** | Manejo de documentos complejos |\n",
    "| **CaracterÃ­sticas** | Ãndices avanzados, soporte PDF/tablas |\n",
    "| **Ideal para** | Repositorios documentales, papers cientÃ­ficos |\n",
    "\n",
    "### Haystack\n",
    "\n",
    "Framework europeo con enfoque en pipelines modulares.\n",
    "\n",
    "| Aspecto | Detalle |\n",
    "|---------|----------|\n",
    "| **Licencia** | Apache 2.0 |\n",
    "| **Origen** | Alemania (deepset) |\n",
    "| **Fortaleza** | Pipelines, sensibilidad RGPD |\n",
    "| **Ideal para** | Enterprise, requisitos de auditorÃ­a |\n",
    "\n",
    "## 9.2 Aplicaciones Listas para Usar\n",
    "\n",
    "### Open WebUI\n",
    "\n",
    "Interfaz web similar a ChatGPT, self-hosted.\n",
    "\n",
    "```bash\n",
    "# Despliegue rÃ¡pido con Docker\n",
    "docker run -d -p 3000:8080 -v open-webui:/app/backend/data \\\n",
    "  -e OLLAMA_BASE_URL=http://host.docker.internal:11434 \\\n",
    "  ghcr.io/open-webui/open-webui:main\n",
    "```\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- RAG integrado (subida de documentos)\n",
    "- GestiÃ³n de usuarios y permisos\n",
    "- MÃºltiples modelos\n",
    "- Interfaz intuitiva\n",
    "\n",
    "### PrivateGPT\n",
    "\n",
    "100% offline, mÃ¡xima privacidad.\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- Sin conexiÃ³n a internet requerida\n",
    "- API compatible con OpenAI\n",
    "- Ideal para datos confidenciales\n",
    "\n",
    "### AnythingLLM\n",
    "\n",
    "Enfoque empresarial con multi-tenancy.\n",
    "\n",
    "**CaracterÃ­sticas:**\n",
    "- Espacios de trabajo separados\n",
    "- MÃºltiples proveedores de LLM\n",
    "- GestiÃ³n de permisos granular\n",
    "\n",
    "## 9.3 Modelos LLM Open Source\n",
    "\n",
    "| Modelo | TamaÃ±os | Licencia | Fortalezas |\n",
    "|--------|---------|----------|------------|\n",
    "| **Llama 3.1** | 8B, 70B, 405B | Llama 3 | PropÃ³sito general, multilingÃ¼e |\n",
    "| **Mistral/Mixtral** | 7B, 8x7B, 8x22B | Apache 2.0 | Eficiencia, razonamiento |\n",
    "| **Qwen 2.5** | 0.5B-72B | Apache 2.0 | MultilingÃ¼e, cÃ³digo |\n",
    "| **Gemma 2** | 2B, 9B, 27B | Gemma | Ligero, edge computing |\n",
    "\n",
    "## 9.4 Servidores de Inferencia\n",
    "\n",
    "### Ollama\n",
    "\n",
    "La forma mÃ¡s simple de ejecutar LLMs localmente.\n",
    "\n",
    "```bash\n",
    "# InstalaciÃ³n\n",
    "curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Descargar y ejecutar un modelo\n",
    "ollama pull llama3.1:8b\n",
    "ollama run llama3.1:8b\n",
    "```\n",
    "\n",
    "### vLLM\n",
    "\n",
    "Alto rendimiento para producciÃ³n.\n",
    "\n",
    "```bash\n",
    "# Servidor de inferencia\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "  --model meta-llama/Llama-3.1-8B-Instruct\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"10-produccion\"></a>\n",
    "# 10. Despliegue en ProducciÃ³n\n",
    "\n",
    "## 10.1 Checklist de ProducciÃ³n\n",
    "\n",
    "### PreparaciÃ³n de Datos\n",
    "\n",
    "- [ ] Chunking optimizado para el dominio\n",
    "- [ ] Metadatos enriquecidos (fuente, fecha, autor)\n",
    "- [ ] Limpieza de texto (OCR, formatos)\n",
    "- [ ] Pipeline de actualizaciÃ³n definido\n",
    "\n",
    "### Retrieval\n",
    "\n",
    "- [ ] Hybrid search implementado si hay tÃ©rminos tÃ©cnicos\n",
    "- [ ] Reranking para alta precisiÃ³n\n",
    "- [ ] Filtros de metadatos configurados\n",
    "- [ ] NÃºmero Ã³ptimo de documentos (k) validado\n",
    "\n",
    "### GeneraciÃ³n\n",
    "\n",
    "- [ ] Prompts probados y optimizados\n",
    "- [ ] Manejo de \"no sÃ©\" implementado\n",
    "- [ ] Citaciones funcionando correctamente\n",
    "- [ ] Guardrails contra alucinaciones\n",
    "\n",
    "### Infraestructura\n",
    "\n",
    "- [ ] AutenticaciÃ³n (LDAP/SSO)\n",
    "- [ ] Cifrado (TLS en trÃ¡nsito, en reposo)\n",
    "- [ ] Logs de auditorÃ­a\n",
    "- [ ] Backups de vectorstore\n",
    "- [ ] MonitorizaciÃ³n de mÃ©tricas\n",
    "\n",
    "### EvaluaciÃ³n\n",
    "\n",
    "- [ ] Conjunto de test creado\n",
    "- [ ] MÃ©tricas baseline establecidas\n",
    "- [ ] Sistema de feedback de usuarios\n",
    "- [ ] A/B testing configurado\n",
    "\n",
    "## 10.2 Arquitectura Recomendada\n",
    "\n",
    "### Stack BÃ¡sico (Para Empezar)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    STACK BÃSICO                         â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LLM:        Llama 3.1 8B o Qwen 2.5 7B (via Ollama)    â”‚\n",
    "â”‚ Embeddings: paraphrase-multilingual-MiniLM o BGE-M3    â”‚\n",
    "â”‚ VectorDB:   ChromaDB o Qdrant                          â”‚\n",
    "â”‚ Framework:  LangChain o LlamaIndex                     â”‚\n",
    "â”‚ Interfaz:   Open WebUI o Gradio                        â”‚\n",
    "â”‚ Hardware:   1x GPU 16GB (RTX 4090, A4000)              â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Stack Avanzado (Escala)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                   STACK AVANZADO                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LLM:         Llama 3.1 70B o Mixtral 8x22B             â”‚\n",
    "â”‚ Servidor:    vLLM con balanceo de carga                â”‚\n",
    "â”‚ VectorDB:    Qdrant cluster o Milvus                   â”‚\n",
    "â”‚ OrquestaciÃ³n: Kubernetes + Helm                        â”‚\n",
    "â”‚ CachÃ©:       Redis para respuestas frecuentes          â”‚\n",
    "â”‚ MonitorizaciÃ³n: Prometheus + Grafana                    â”‚\n",
    "â”‚ Hardware:    MÃºltiples GPUs A100/H100                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## 10.3 PrÃ³ximos Pasos\n",
    "\n",
    "1. **Prueba de Concepto**\n",
    "   - Instalar Ollama + Open WebUI\n",
    "   - Probar con un conjunto pequeÃ±o de documentos\n",
    "   - Validar con usuarios piloto\n",
    "\n",
    "2. **IteraciÃ³n**\n",
    "   - Crear conjunto de evaluaciÃ³n\n",
    "   - Optimizar chunking y prompts\n",
    "   - Implementar tÃ©cnicas avanzadas segÃºn necesidad\n",
    "\n",
    "3. **ProducciÃ³n**\n",
    "   - Implementar seguridad y autenticaciÃ³n\n",
    "   - Configurar monitorizaciÃ³n\n",
    "   - Establecer procesos de actualizaciÃ³n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# ğŸ¯ ConclusiÃ³n y Recursos\n",
    "\n",
    "## Resumen de lo Aprendido\n",
    "\n",
    "En esta guÃ­a hemos cubierto:\n",
    "\n",
    "1. **Fundamentos**: QuÃ© es RAG, por quÃ© es necesario, arquitectura\n",
    "2. **Embeddings**: RepresentaciÃ³n semÃ¡ntica, modelos, mÃ©tricas\n",
    "3. **Chunking**: Estrategias, parÃ¡metros Ã³ptimos, metadatos\n",
    "4. **Vector Stores**: ChromaDB, FAISS, bÃºsqueda y filtros\n",
    "5. **Pipeline RAG**: ConstrucciÃ³n completa paso a paso\n",
    "6. **TÃ©cnicas Avanzadas**: Hybrid search, reranking, multi-query\n",
    "7. **EvaluaciÃ³n**: MÃ©tricas de retrieval y generaciÃ³n\n",
    "8. **Prompting**: Plantillas y tÃ©cnicas efectivas\n",
    "9. **Soluciones Open Source**: Frameworks y aplicaciones\n",
    "10. **ProducciÃ³n**: Checklist y arquitecturas recomendadas\n",
    "\n",
    "## Recursos Adicionales\n",
    "\n",
    "### DocumentaciÃ³n Oficial\n",
    "- LangChain: https://langchain.com\n",
    "- LlamaIndex: https://llamaindex.ai\n",
    "- ChromaDB: https://www.trychroma.com\n",
    "- Ollama: https://ollama.ai\n",
    "\n",
    "### Papers Fundamentales\n",
    "- \"Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks\" (Lewis et al., 2020)\n",
    "- \"A Survey on Retrieval-Augmented Text Generation\" (Gao et al., 2023)\n",
    "\n",
    "### Modelos en Hugging Face\n",
    "- https://huggingface.co/models\n",
    "\n",
    "---\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
